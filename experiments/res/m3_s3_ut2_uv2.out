0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.958382033276095, agent episode reward: [0.8857947062323072, -26.844176739508402], time: 41.462
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.107386271652317, agent episode reward: [-2.948459584395762, -19.15892668725656], time: 54.471
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.59108187579201, agent episode reward: [-5.439821438782626, -8.151260437009384], time: 53.73
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -11.392703536554396, agent episode reward: [-4.232058381935128, -7.160645154619266], time: 54.483
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -10.832737591104216, agent episode reward: [-3.101581072076041, -7.731156519028175], time: 53.544
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -10.446416385101266, agent episode reward: [-2.9616628713965705, -7.484753513704695], time: 53.441
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -10.564113702450548, agent episode reward: [-3.0440402731318588, -7.520073429318689], time: 53.369
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -10.324021502914366, agent episode reward: [-2.8063143967517905, -7.517707106162575], time: 54.97
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -10.632088254320351, agent episode reward: [-2.905671882702168, -7.726416371618184], time: 53.703
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -10.48093388498651, agent episode reward: [-2.6755854108059585, -7.8053484741805494], time: 53.435
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -10.358585502319304, agent episode reward: [-2.6724955693027668, -7.686089933016535], time: 54.847
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -10.46791499024747, agent episode reward: [-2.8714382322132543, -7.596476758034217], time: 53.768
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -10.213321067123262, agent episode reward: [-2.429912446115827, -7.7834086210074345], time: 53.99
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -10.08602209558361, agent episode reward: [-2.241532981689363, -7.844489113894247], time: 54.158
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -10.049304601930839, agent episode reward: [-2.1482269304231094, -7.901077671507728], time: 54.618
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -9.567570408826889, agent episode reward: [-1.8755512111613426, -7.692019197665546], time: 55.015
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -9.8927622007229, agent episode reward: [-2.053849351108411, -7.838912849614491], time: 56.476
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -10.188447115687916, agent episode reward: [-2.072633922025977, -8.115813193661939], time: 54.848
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -10.190558827294211, agent episode reward: [-2.3440601946601802, -7.846498632634029], time: 54.69
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -10.155536843399423, agent episode reward: [-1.9116095278423533, -8.24392731555707], time: 53.85
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -10.177871681519777, agent episode reward: [-2.1785999628270383, -7.9992717186927385], time: 53.582
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -10.257225713850241, agent episode reward: [-2.3779567108734465, -7.8792690029767956], time: 53.858
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -10.384135184516174, agent episode reward: [-2.2892709324607696, -8.094864252055403], time: 54.848
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -10.223821676003237, agent episode reward: [-2.0383006303272717, -8.185521045675966], time: 53.796
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -10.703644248387365, agent episode reward: [-2.2941754255432754, -8.409468822844092], time: 54.06
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -10.383259148733936, agent episode reward: [-1.411614122290455, -8.97164502644348], time: 55.017
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -10.538483630567105, agent episode reward: [-1.2507868787587684, -9.287696751808337], time: 54.431
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -10.243907966949351, agent episode reward: [-0.8036084533657586, -9.440299513583593], time: 54.259
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -10.728121715143892, agent episode reward: [-1.7717077807145838, -8.956413934429309], time: 55.096
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -10.392366743773541, agent episode reward: [-1.7708422280162148, -8.621524515757327], time: 54.995
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -10.527065605855524, agent episode reward: [-1.758658403782737, -8.768407202072789], time: 54.68
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -10.823440221037014, agent episode reward: [-1.6218169669392777, -9.201623254097735], time: 53.888
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -10.456701733503698, agent episode reward: [-1.264431583720072, -9.192270149783628], time: 54.246
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -10.288357985313477, agent episode reward: [-1.051698633228054, -9.236659352085422], time: 54.143
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -9.96568979206133, agent episode reward: [-1.1908014557662951, -8.774888336295037], time: 54.174
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -9.993293785654357, agent episode reward: [-1.1610987632504302, -8.832195022403926], time: 54.298
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -10.061076430844503, agent episode reward: [-1.2977023478700138, -8.76337408297449], time: 54.892
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -10.17896814701262, agent episode reward: [-1.33963025427613, -8.83933789273649], time: 54.083
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -9.789224604947112, agent episode reward: [-0.48130238745098314, -9.307922217496131], time: 53.804
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -9.814137327744612, agent episode reward: [-0.43121286169547535, -9.382924466049136], time: 53.82
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -9.573715053022951, agent episode reward: [-0.2727388164365617, -9.30097623658639], time: 53.9
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -10.089570917262348, agent episode reward: [-0.6019993575786722, -9.487571559683678], time: 54.884
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -10.058303856733994, agent episode reward: [-0.7976555319379734, -9.26064832479602], time: 54.278
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -10.208405757626904, agent episode reward: [-0.404386005845523, -9.804019751781382], time: 53.908
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -10.378247255309306, agent episode reward: [-0.7330389787996108, -9.645208276509697], time: 54.026
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -10.46668940095069, agent episode reward: [-1.2696336872822427, -9.197055713668448], time: 55.165
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -9.843485051948253, agent episode reward: [-1.6441014739451663, -8.199383578003086], time: 55.487
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -10.009670134443976, agent episode reward: [-1.7293175446170916, -8.280352589826885], time: 56.643
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -9.79831120577831, agent episode reward: [-1.940304411433257, -7.8580067943450524], time: 56.234
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -9.889232622786334, agent episode reward: [-2.1592861205408242, -7.729946502245509], time: 57.49
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -9.700543869792805, agent episode reward: [-2.238068751871722, -7.462475117921083], time: 56.412
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -10.32344130481518, agent episode reward: [-2.4417379988821533, -7.881703305933028], time: 55.07
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -10.441801237213818, agent episode reward: [-3.0401366221118264, -7.40166461510199], time: 57.146
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -10.808812774114243, agent episode reward: [-3.2362211105018397, -7.572591663612402], time: 55.753
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -10.682890170143994, agent episode reward: [-3.2743715340884507, -7.408518636055542], time: 56.904
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -10.300784204973043, agent episode reward: [-2.9024504990786784, -7.398333705894366], time: 57.391
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -10.342882817665828, agent episode reward: [-2.801086120817583, -7.541796696848244], time: 57.555
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -10.716399851842445, agent episode reward: [-3.117812771121152, -7.598587080721294], time: 56.94
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -10.79145497289503, agent episode reward: [-3.345842243429786, -7.445612729465243], time: 55.983
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -10.476412886730609, agent episode reward: [-2.879455623041015, -7.596957263689593], time: 56.06
...Finished total of 60001 episodes.
