0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.444364671476674, agent episode reward: [-24.132444572418166, -0.6559600495292546, -0.6559600495292546], time: 59.332
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -22.356333266152287, agent episode reward: [-21.4904985635754, -0.4329173512884443, -0.4329173512884443], time: 77.982
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -0.4456109846800271, agent episode reward: [-6.460184484351442, 3.0072867498357083, 3.0072867498357083], time: 78.698
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -6.264378560604671, agent episode reward: [-2.499391045917134, -1.882493757343769, -1.882493757343769], time: 78.379
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -7.840393473983231, agent episode reward: [-5.686030963093266, -1.0771812554449822, -1.0771812554449822], time: 78.625
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -13.627139713556248, agent episode reward: [-16.333081845939166, 1.3529710661914578, 1.3529710661914578], time: 78.892
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -19.552957870850843, agent episode reward: [-23.454175903876568, 1.950609016512862, 1.950609016512862], time: 78.591
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 1.3544769509172978, agent episode reward: [-19.51547014344726, 10.434973547182278, 10.434973547182278], time: 79.083
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 8.5043508319611, agent episode reward: [-12.685073212388524, 10.594712022174813, 10.594712022174813], time: 79.718
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 7.892891047791856, agent episode reward: [-13.077348282784198, 10.485119665288027, 10.485119665288027], time: 78.418
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 10.487915047888404, agent episode reward: [-17.542364120672364, 14.015139584280385, 14.015139584280385], time: 79.388
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 4.698695754510029, agent episode reward: [-20.90315472843581, 12.80092524147292, 12.80092524147292], time: 80.235
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 1.975109381784136, agent episode reward: [-22.710440309238315, 12.342774845511226, 12.342774845511226], time: 79.895
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -0.4035471629259464, agent episode reward: [-22.401396553953454, 10.998924695513754, 10.998924695513754], time: 81.005
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -1.6245122034818165, agent episode reward: [-20.64336169706745, 9.509424746792819, 9.509424746792819], time: 79.533
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 1.5273008870058609, agent episode reward: [-21.229839065164622, 11.37856997608524, 11.37856997608524], time: 78.599
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -0.2689236916661466, agent episode reward: [-21.526869455454733, 10.628972881894294, 10.628972881894294], time: 79.123
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -2.6902598753608897, agent episode reward: [-20.441588683268098, 8.875664403953605, 8.875664403953605], time: 78.699
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -4.126687247831445, agent episode reward: [-19.6140589097298, 7.743685830949176, 7.743685830949176], time: 79.587
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -0.8476426892478524, agent episode reward: [-18.7015383873383, 8.926947849045224, 8.926947849045224], time: 79.845
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -0.05451750286464619, agent episode reward: [-20.72124409366792, 10.333363295401638, 10.333363295401638], time: 79.109
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 2.201369135303589, agent episode reward: [-20.61879732651923, 11.41008323091141, 11.41008323091141], time: 78.996
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 5.281370505024428, agent episode reward: [-20.621378734425956, 12.951374619725195, 12.951374619725195], time: 79.749
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 3.034312861852834, agent episode reward: [-20.733134123975614, 11.883723492914223, 11.883723492914223], time: 80.435
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 1.9494444071795156, agent episode reward: [-20.226625072889387, 11.08803474003445, 11.08803474003445], time: 79.012
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 4.057976788802353, agent episode reward: [-21.636943291078634, 12.847460039940495, 12.847460039940495], time: 79.097
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -1.6827874137265648, agent episode reward: [-21.555270805746954, 9.936241696010194, 9.936241696010194], time: 79.003
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -1.1724258200896178, agent episode reward: [-21.073024444046872, 9.950299311978625, 9.950299311978625], time: 79.898
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 1.0451946501012943, agent episode reward: [-20.88697000559871, 10.966082327850003, 10.966082327850003], time: 79.437
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 5.061479419964726, agent episode reward: [-20.6133947649123, 12.837437092438515, 12.837437092438515], time: 79.79
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 1.737655679944327, agent episode reward: [-20.33199095490914, 11.034823317426733, 11.034823317426733], time: 79.408
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 2.6732238373076216, agent episode reward: [-20.6218340650224, 11.647528951165011, 11.647528951165011], time: 79.111
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -0.6514364471785484, agent episode reward: [-21.201560757325694, 10.275062155073574, 10.275062155073574], time: 78.782
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -2.6677861168564787, agent episode reward: [-21.41171808425512, 9.371965983699322, 9.371965983699322], time: 79.228
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 0.10528006208277077, agent episode reward: [-21.316093303519462, 10.710686682801116, 10.710686682801116], time: 81.257
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -1.0332316920956777, agent episode reward: [-18.06151868514067, 8.514143496522498, 8.514143496522498], time: 81.31
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -1.113784047297533, agent episode reward: [-19.831338206587084, 9.358777079644774, 9.358777079644774], time: 82.319
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -8.182354087062075, agent episode reward: [-16.432727126894054, 4.125186519915989, 4.125186519915989], time: 82.563
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -15.661054916396127, agent episode reward: [-13.045929758911402, -1.3075625787423633, -1.3075625787423633], time: 80.689
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -10.02399985656186, agent episode reward: [-11.582454408354533, 0.779227275896337, 0.779227275896337], time: 82.967
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -12.155220423205948, agent episode reward: [-14.191014920278342, 1.0178972485361957, 1.0178972485361957], time: 79.859
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -11.99805258882384, agent episode reward: [-24.323318546004586, 6.1626329785903735, 6.1626329785903735], time: 76.21
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -9.712002958085522, agent episode reward: [-24.956639548370582, 7.622318295142529, 7.622318295142529], time: 75.317
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -3.5336908678590557, agent episode reward: [-23.970546753186177, 10.218427942663562, 10.218427942663562], time: 77.678
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 0.28188717805419766, agent episode reward: [-26.70849823433947, 13.495192706196834, 13.495192706196834], time: 75.26
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 5.967515542887221, agent episode reward: [-25.79552229595004, 15.881518919418632, 15.881518919418632], time: 73.582
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 13.863683915110249, agent episode reward: [-28.854535279640025, 21.359109597375138, 21.359109597375138], time: 73.702
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 13.131245929629728, agent episode reward: [-24.48899554149386, 18.810120735561796, 18.810120735561796], time: 75.397
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 14.412489389310755, agent episode reward: [-23.927571106378338, 19.170030247844544, 19.170030247844544], time: 73.994
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 15.798374010569507, agent episode reward: [-22.43135069580822, 19.114862353188865, 19.114862353188865], time: 75.306
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 15.235706372215729, agent episode reward: [-27.31398191619011, 21.274844144202923, 21.274844144202923], time: 73.952
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 10.681449445362505, agent episode reward: [-24.187464994410643, 17.43445721988657, 17.43445721988657], time: 76.752
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 14.672127397031932, agent episode reward: [-25.722067363505477, 20.19709738026871, 20.19709738026871], time: 75.61
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 8.522955895650512, agent episode reward: [-29.1303019175328, 18.826628906591655, 18.826628906591655], time: 73.89
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 4.5613846529121265, agent episode reward: [-27.103571805715323, 15.832478229313725, 15.832478229313725], time: 73.847
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 0.6463925634784525, agent episode reward: [-26.35822175685902, 13.502307160168737, 13.502307160168737], time: 73.877
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -0.6702022610669287, agent episode reward: [-25.48529331230436, 12.407545525618715, 12.407545525618715], time: 74.846
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -0.2705242825563855, agent episode reward: [-26.130977037468263, 12.93022637745594, 12.93022637745594], time: 75.103
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 2.1806172828836967, agent episode reward: [-26.777783888593046, 14.479200585738374, 14.479200585738374], time: 77.233
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 0.2524472927541086, agent episode reward: [-25.707722818969305, 12.980085055861705, 12.980085055861705], time: 74.863
...Finished total of 60001 episodes.
