0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -2.474728683131645, agent episode reward: [1.74, 1.74, 1.74, -7.694728683131644], time: 121.231
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -1.972489425981848, agent episode reward: [3.4, 3.4, 3.4, -12.172489425981846], time: 156.948
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 8.50723411662379, agent episode reward: [4.74, 4.74, 4.74, -5.7127658833762105], time: 156.043
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 10.978173465421783, agent episode reward: [5.77, 5.77, 5.77, -6.331826534578215], time: 155.828
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 13.319469892957052, agent episode reward: [6.87, 6.87, 6.87, -7.290530107042949], time: 156.33
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 12.765327288522544, agent episode reward: [6.6, 6.6, 6.6, -7.034672711477457], time: 156.247
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 13.700210535085738, agent episode reward: [7.03, 7.03, 7.03, -7.389789464914262], time: 156.984
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 16.074665959341974, agent episode reward: [8.48, 8.48, 8.48, -9.365334040658025], time: 155.242
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 21.65516665818687, agent episode reward: [11.69, 11.69, 11.69, -13.41483334181313], time: 156.798
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 29.797923884696864, agent episode reward: [15.22, 15.22, 15.22, -15.862076115303132], time: 155.7
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 35.47310956927197, agent episode reward: [18.63, 18.63, 18.63, -20.41689043072803], time: 156.463
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 38.364196764962, agent episode reward: [19.86, 19.86, 19.86, -21.215803235037995], time: 156.203
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 31.50643423465981, agent episode reward: [16.88, 16.88, 16.88, -19.13356576534019], time: 156.013
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 36.477030276576, agent episode reward: [19.27, 19.27, 19.27, -21.332969723423997], time: 155.935
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 35.02792234470126, agent episode reward: [19.03, 19.03, 19.03, -22.062077655298744], time: 160.673
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 36.87861206784005, agent episode reward: [20.8, 20.8, 20.8, -25.521387932159946], time: 164.981
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 25.92505782668875, agent episode reward: [16.28, 16.28, 16.28, -22.914942173311253], time: 165.73
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 18.588296063456713, agent episode reward: [13.57, 13.57, 13.57, -22.121703936543287], time: 165.807
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 14.812747304758783, agent episode reward: [11.69, 11.69, 11.69, -20.257252695241217], time: 165.072
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 10.483485440701854, agent episode reward: [8.92, 8.92, 8.92, -16.276514559298146], time: 165.162
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 11.599653227719728, agent episode reward: [8.6, 8.6, 8.6, -14.200346772280271], time: 164.82
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 14.841811938291654, agent episode reward: [10.0, 10.0, 10.0, -15.158188061708346], time: 165.205
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 13.83989901157365, agent episode reward: [9.12, 9.12, 9.12, -13.520100988426352], time: 166.415
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 11.711329547084283, agent episode reward: [7.56, 7.56, 7.56, -10.968670452915719], time: 164.213
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 16.97046668683781, agent episode reward: [10.07, 10.07, 10.07, -13.239533313162191], time: 165.763
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 18.0126678498276, agent episode reward: [10.49, 10.49, 10.49, -13.457332150172396], time: 164.811
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 18.63574380129954, agent episode reward: [11.03, 11.03, 11.03, -14.45425619870046], time: 165.602
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 20.283849339269956, agent episode reward: [11.55, 11.55, 11.55, -14.366150660730044], time: 162.997
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 17.708611070591424, agent episode reward: [10.74, 10.74, 10.74, -14.511388929408577], time: 165.529
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 21.20186849006802, agent episode reward: [12.04, 12.04, 12.04, -14.91813150993198], time: 165.156
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 18.318169348392292, agent episode reward: [10.37, 10.37, 10.37, -12.791830651607707], time: 165.382
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 17.51602599364709, agent episode reward: [10.29, 10.29, 10.29, -13.353974006352908], time: 163.339
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 19.77159307898893, agent episode reward: [11.35, 11.35, 11.35, -14.278406921011076], time: 165.425
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 17.839877092442816, agent episode reward: [10.56, 10.56, 10.56, -13.840122907557182], time: 164.532
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 16.442361721785364, agent episode reward: [10.04, 10.04, 10.04, -13.677638278214635], time: 165.312
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 21.00561555393258, agent episode reward: [12.02, 12.02, 12.02, -15.054384446067424], time: 166.451
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 20.046068790681193, agent episode reward: [11.26, 11.26, 11.26, -13.733931209318806], time: 165.514
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 20.787544668509565, agent episode reward: [11.7, 11.7, 11.7, -14.312455331490437], time: 164.465
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 20.113472605028818, agent episode reward: [11.45, 11.45, 11.45, -14.236527394971185], time: 165.744
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 19.712774883290667, agent episode reward: [11.29, 11.29, 11.29, -14.157225116709334], time: 164.99
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 19.500689010262803, agent episode reward: [10.86, 10.86, 10.86, -13.079310989737198], time: 164.942
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 22.591494255388223, agent episode reward: [12.36, 12.36, 12.36, -14.488505744611775], time: 164.743
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 21.594080844003038, agent episode reward: [12.19, 12.19, 12.19, -14.975919155996964], time: 165.705
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 19.911760603678886, agent episode reward: [11.2, 11.2, 11.2, -13.688239396321114], time: 166.132
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 23.226796915894802, agent episode reward: [13.02, 13.02, 13.02, -15.833203084105197], time: 166.367
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 23.37087236812481, agent episode reward: [13.06, 13.06, 13.06, -15.809127631875187], time: 165.517
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 26.494299763371842, agent episode reward: [14.73, 14.73, 14.73, -17.69570023662816], time: 163.815
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 22.688676248968513, agent episode reward: [12.64, 12.64, 12.64, -15.231323751031491], time: 164.718
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 20.067048456529548, agent episode reward: [11.73, 11.73, 11.73, -15.122951543470455], time: 165.309
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 19.88488875451798, agent episode reward: [11.85, 11.85, 11.85, -15.665111245482018], time: 165.166
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 22.943282094753638, agent episode reward: [13.26, 13.26, 13.26, -16.836717905246363], time: 165.61
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 20.601601737951537, agent episode reward: [11.91, 11.91, 11.91, -15.128398262048465], time: 165.397
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 24.278118024033912, agent episode reward: [13.62, 13.62, 13.62, -16.581881975966088], time: 164.991
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 23.34796281361656, agent episode reward: [13.69, 13.69, 13.69, -17.722037186383435], time: 165.253
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 30.309905391344763, agent episode reward: [16.84, 16.84, 16.84, -20.21009460865523], time: 164.047
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 36.81389673655179, agent episode reward: [20.47, 20.47, 20.47, -24.596103263448203], time: 159.345
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 47.4589312083125, agent episode reward: [25.52, 25.52, 25.52, -29.1010687916875], time: 156.635
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 41.651448245036924, agent episode reward: [22.5, 22.5, 22.5, -25.848551754963083], time: 153.49
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 38.360947002562064, agent episode reward: [20.99, 20.99, 20.99, -24.609052997437935], time: 151.857
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 35.619412744819854, agent episode reward: [19.51, 19.51, 19.51, -22.910587255180147], time: 137.584
...Finished total of 60001 episodes.
