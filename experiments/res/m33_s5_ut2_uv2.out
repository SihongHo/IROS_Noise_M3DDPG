0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -5.8223174806988665, agent episode reward: [2.32, 2.32, 2.32, -12.782317480698866], time: 121.609
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -10.108444472610165, agent episode reward: [3.9, 3.9, 3.9, -21.808444472610166], time: 156.359
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 7.941989349943075, agent episode reward: [4.61, 4.61, 4.61, -5.888010650056925], time: 156.056
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 11.136466990931591, agent episode reward: [5.84, 5.84, 5.84, -6.383533009068408], time: 154.681
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 12.532823733051858, agent episode reward: [6.4, 6.4, 6.4, -6.667176266948141], time: 155.881
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 12.661861909468092, agent episode reward: [6.5, 6.5, 6.5, -6.838138090531908], time: 156.249
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 14.07185752353364, agent episode reward: [7.25, 7.25, 7.25, -7.678142476466362], time: 155.802
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 15.181097503288271, agent episode reward: [7.79, 7.79, 7.79, -8.188902496711732], time: 155.118
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 13.71085303967023, agent episode reward: [7.11, 7.11, 7.11, -7.619146960329769], time: 157.42
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 15.51353604565475, agent episode reward: [8.01, 8.01, 8.01, -8.516463954345248], time: 156.058
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 15.636777672782136, agent episode reward: [8.16, 8.16, 8.16, -8.843222327217864], time: 155.956
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 28.02143910665765, agent episode reward: [14.33, 14.33, 14.33, -14.968560893342351], time: 156.565
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 42.42892786277217, agent episode reward: [21.93, 21.93, 21.93, -23.361072137227826], time: 156.763
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 48.59774921624953, agent episode reward: [25.78, 25.78, 25.78, -28.742250783750467], time: 156.661
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 55.70472281567641, agent episode reward: [29.01, 29.01, 29.01, -31.325277184323586], time: 160.922
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 39.01779803926531, agent episode reward: [21.57, 21.57, 21.57, -25.692201960734693], time: 163.83
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 25.33110877565328, agent episode reward: [15.21, 15.21, 15.21, -20.298891224346722], time: 164.938
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 16.451917542668156, agent episode reward: [11.5, 11.5, 11.5, -18.048082457331844], time: 165.75
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 11.211824814607713, agent episode reward: [9.64, 9.64, 9.64, -17.70817518539229], time: 163.342
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 17.568966556426844, agent episode reward: [12.31, 12.31, 12.31, -19.361033443573156], time: 164.218
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 10.710577065219798, agent episode reward: [9.08, 9.08, 9.08, -16.529422934780204], time: 165.209
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 17.52286960509997, agent episode reward: [11.94, 11.94, 11.94, -18.29713039490003], time: 165.265
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 19.31394178077689, agent episode reward: [12.42, 12.42, 12.42, -17.946058219223108], time: 165.909
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 20.0809489091255, agent episode reward: [12.7, 12.7, 12.7, -18.019051090874502], time: 165.592
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 19.849507395180538, agent episode reward: [12.18, 12.18, 12.18, -16.69049260481946], time: 164.618
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 24.559482421950392, agent episode reward: [14.38, 14.38, 14.38, -18.580517578049605], time: 163.181
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 20.261133411153054, agent episode reward: [12.36, 12.36, 12.36, -16.818866588846948], time: 164.607
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 18.66801677643678, agent episode reward: [11.73, 11.73, 11.73, -16.521983223563225], time: 164.666
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 15.482220150542476, agent episode reward: [10.7, 10.7, 10.7, -16.617779849457527], time: 165.235
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 11.783513514936418, agent episode reward: [8.61, 8.61, 8.61, -14.046486485063582], time: 166.322
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 17.332803704625235, agent episode reward: [11.65, 11.65, 11.65, -17.617196295374765], time: 164.321
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 19.25561039118204, agent episode reward: [12.4, 12.4, 12.4, -17.944389608817957], time: 164.091
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 20.245024082640033, agent episode reward: [13.26, 13.26, 13.26, -19.534975917359965], time: 165.824
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 15.633100305473043, agent episode reward: [10.4, 10.4, 10.4, -15.566899694526958], time: 165.125
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 17.908845021644584, agent episode reward: [11.53, 11.53, 11.53, -16.681154978355416], time: 165.636
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 15.38641974930135, agent episode reward: [10.17, 10.17, 10.17, -15.12358025069865], time: 166.458
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 20.59768641765329, agent episode reward: [12.35, 12.35, 12.35, -16.452313582346708], time: 164.486
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 19.116963357577365, agent episode reward: [11.85, 11.85, 11.85, -16.43303664242264], time: 165.44
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 24.083246538737118, agent episode reward: [14.24, 14.24, 14.24, -18.63675346126288], time: 163.721
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 26.825941533372134, agent episode reward: [15.25, 15.25, 15.25, -18.924058466627866], time: 164.271
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 27.034057868970987, agent episode reward: [15.39, 15.39, 15.39, -19.135942131029015], time: 163.777
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 26.351532838320214, agent episode reward: [15.39, 15.39, 15.39, -19.818467161679784], time: 165.075
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 26.453219140425112, agent episode reward: [15.3, 15.3, 15.3, -19.446780859574886], time: 165.423
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 26.145480659836394, agent episode reward: [15.54, 15.54, 15.54, -20.474519340163607], time: 165.253
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 27.30804930283548, agent episode reward: [16.12, 16.12, 16.12, -21.051950697164518], time: 165.767
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 35.433063170055554, agent episode reward: [19.84, 19.84, 19.84, -24.086936829944438], time: 165.206
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 26.08818991752261, agent episode reward: [15.83, 15.83, 15.83, -21.40181008247739], time: 165.942
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 28.356803616200317, agent episode reward: [16.87, 16.87, 16.87, -22.253196383799686], time: 166.104
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 28.692903632667566, agent episode reward: [17.04, 17.04, 17.04, -22.42709636733243], time: 165.432
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 27.1193750969254, agent episode reward: [16.27, 16.27, 16.27, -21.6906249030746], time: 163.631
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 27.58810069305992, agent episode reward: [16.07, 16.07, 16.07, -20.62189930694008], time: 165.883
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 31.353609453324253, agent episode reward: [17.92, 17.92, 17.92, -22.406390546675745], time: 165.319
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 28.403210907428452, agent episode reward: [16.82, 16.82, 16.82, -22.05678909257155], time: 165.944
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 38.81578777117766, agent episode reward: [21.36, 21.36, 21.36, -25.26421222882234], time: 165.965
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 34.59792325098801, agent episode reward: [19.71, 19.71, 19.71, -24.532076749011985], time: 164.938
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 33.22849146859761, agent episode reward: [19.12, 19.12, 19.12, -24.131508531402385], time: 160.493
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 34.85521480576481, agent episode reward: [19.41, 19.41, 19.41, -23.374785194235187], time: 156.635
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 34.53541584459775, agent episode reward: [19.39, 19.39, 19.39, -23.634584155402255], time: 152.295
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 38.062908362517746, agent episode reward: [20.85, 20.85, 20.85, -24.487091637482262], time: 151.778
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 33.62683402462758, agent episode reward: [18.7, 18.7, 18.7, -22.47316597537242], time: 134.718
...Finished total of 60001 episodes.
