0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
3 bad agents
      adv rate for q_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
4 good agents
      adv rate for q_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
5 good agents
      adv rate for q_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 4 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.100132073456034, agent episode reward: [0.624771711057902, 0.7520549685245449, 0.7215902412772969, 0.6856959838814741, -13.41745193429908, -14.466793043898173], time: 249.009
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -32.15697426552534, agent episode reward: [2.416951719166518, 2.4550309075227608, 2.1366916811904684, 2.5641177789748264, -13.168819541205762, -28.56094681117416], time: 319.674
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 12.243625315315175, agent episode reward: [4.679389802640451, 4.611933315722023, 4.378062370647905, 4.494429547698698, -2.835895786532491, -3.0842939348614125], time: 322.964
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 14.797474640389417, agent episode reward: [5.253059164676336, 4.955565809063133, 5.033918126180624, 4.830901970049838, -2.628060112782434, -2.6479103167980815], time: 327.661
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 21.01639999829853, agent episode reward: [7.188074346123965, 6.8521237957834815, 7.017368998948048, 6.702672360959397, -2.9281992930213994, -3.8156402104949585], time: 327.987
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 23.33955701498798, agent episode reward: [8.011735236063176, 7.3405362483590215, 7.645650122201522, 7.698141090844615, -3.430367579998383, -3.926138102481968], time: 330.051
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 27.900836896478303, agent episode reward: [9.401071472142336, 8.823700892492175, 9.002497229578752, 9.297452897394717, -4.614522692959556, -4.009362902170119], time: 329.801
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 33.074732603748096, agent episode reward: [10.974956987513128, 10.59411486694836, 10.563274229042658, 10.970130786998016, -4.283052001237126, -5.744692265516942], time: 327.489
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 39.756755887036505, agent episode reward: [13.186704255735963, 12.963642954883275, 12.922316639148182, 13.219408505073313, -5.534403083467411, -7.0009133843368145], time: 329.009
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 37.59393192308243, agent episode reward: [12.525521417354081, 12.28318579961648, 12.436334837046894, 12.556318630915365, -5.284454014181454, -6.9229747476689285], time: 327.986
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 39.04810043938879, agent episode reward: [12.904261240779586, 12.680070381736092, 12.779276482668203, 12.892135020380717, -6.281487981956444, -5.926154704219358], time: 326.846
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 42.24250288473924, agent episode reward: [13.99448756568181, 13.917920616656822, 13.93233698723673, 14.028799641198775, -6.161799083734607, -7.4692428423002974], time: 330.467
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 52.56139804129754, agent episode reward: [17.529784437290292, 17.518327787026383, 17.58241788290218, 17.602613981659267, -7.347330778410703, -10.324415269169892], time: 328.161
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 47.814753238571036, agent episode reward: [16.14088762985603, 16.129480482481014, 16.132478860267213, 16.160342928135247, -6.271384974997476, -10.477051687170986], time: 329.995
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 45.12393151903177, agent episode reward: [15.68866007868489, 15.629470442843072, 15.71878836871429, 15.688043102299485, -7.077929364269525, -10.523101109240445], time: 330.246
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 49.63046680793829, agent episode reward: [16.983815603646892, 16.932845877973577, 16.970899369093704, 16.971820014578697, -6.615170082690672, -11.613743974663917], time: 331.837
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 39.45535391147288, agent episode reward: [14.098741193113858, 14.051488377554808, 14.164311367587302, 14.12831907585867, -7.992784093734647, -8.994722008907106], time: 331.206
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 33.55249093260761, agent episode reward: [12.234341299891414, 12.221913360254755, 12.297425232369324, 12.269014914533232, -6.464663918255219, -9.005539956185896], time: 327.236
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 41.110024462900604, agent episode reward: [14.326044986399745, 14.287151999160532, 14.312845063847123, 14.381753259557922, -6.012007865956062, -10.185762980108647], time: 330.919
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 31.481931569876878, agent episode reward: [11.167960467760398, 11.21417956240972, 11.135692414014535, 11.185931851567059, -4.558558464520187, -8.663274261354648], time: 331.39
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 27.22039001693276, agent episode reward: [10.14977522586848, 10.166907026429227, 10.101802979894718, 10.13504610769484, -4.7228779252018604, -8.610263397752643], time: 327.923
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 16.40659243088254, agent episode reward: [7.025008443906708, 7.004981611132646, 6.919929109548506, 6.986792538780226, -4.320406144366972, -7.2097131281185725], time: 329.325
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 17.26511668257101, agent episode reward: [7.013092774142295, 7.00746070449086, 6.912121022332801, 6.927868375036706, -3.492059052678348, -7.103367140753304], time: 330.683
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 16.579725666762318, agent episode reward: [6.512404897389737, 6.449108750010593, 6.358667483270527, 6.412489349837899, -3.471463955859689, -5.681480857886749], time: 327.301
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 16.41839349340648, agent episode reward: [6.398348891371984, 6.383580225073282, 6.253647537609918, 6.349920894685267, -3.155216017122024, -5.811888038211945], time: 325.854
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 19.125258162631336, agent episode reward: [7.289518450838984, 7.240106546224164, 7.1346839349460875, 7.251202191849971, -3.7036819148889615, -6.086571046338907], time: 329.064
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 18.510578720209278, agent episode reward: [7.362785425893344, 7.21967788090275, 7.191881472636365, 7.2856955480120345, -5.01162636142627, -5.537835245808952], time: 325.845
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 15.762168913539629, agent episode reward: [6.887901149808379, 6.754078007036176, 6.718723550162131, 6.803712416818919, -5.680984579051436, -5.721261631234545], time: 323.532
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 13.994258630956418, agent episode reward: [7.3956849687289345, 7.307616096292851, 7.308401296861692, 7.322515535581377, -7.663059424868454, -7.676899841639982], time: 318.957
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 15.264907519958355, agent episode reward: [7.053233095671014, 6.957475383649397, 7.030822105453471, 6.991352864064434, -5.804097548133503, -6.963878380746459], time: 317.98
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 19.350878901262938, agent episode reward: [8.35276969270847, 8.280209143534194, 8.166760766301278, 8.222779502776852, -6.353922612906555, -7.317717591151301], time: 317.311
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 24.921028667598133, agent episode reward: [9.525147924723301, 9.375435083169519, 9.369248380364972, 9.353892383905928, -4.995348677613862, -7.707346426951724], time: 318.434
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 29.897791642902828, agent episode reward: [10.49067197225054, 10.374893329781589, 10.346465532534186, 10.358529755583444, -3.5535709124673787, -8.119198034779552], time: 318.017
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 29.32119599805317, agent episode reward: [10.434146379374786, 10.247710304379957, 10.390589988340377, 10.44328442243861, -4.720749653964921, -7.473785442515639], time: 318.396
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 26.924996804127495, agent episode reward: [9.557225580396233, 9.305152393855328, 9.42184076037593, 9.438829550121309, -4.36130131459607, -6.436750166025235], time: 318.291
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 27.754969843034917, agent episode reward: [9.642568968692855, 9.471050876842437, 9.599293493642076, 9.53245630380505, -3.6087287977265006, -6.881671002220998], time: 316.928
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 24.69190410055431, agent episode reward: [9.439452001632215, 9.23064777815668, 9.335641611042629, 9.317929918746199, -4.556541681264932, -8.07522552775848], time: 321.021
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 21.656985662586763, agent episode reward: [8.82187615636105, 8.670624758810192, 8.727724425272303, 8.645311888577066, -4.786023099608565, -8.422528466825282], time: 317.381
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 23.09229780554199, agent episode reward: [9.012243494924808, 8.774841656718811, 8.857405386762764, 8.797097440610242, -3.8070839228923616, -8.54220625058228], time: 318.373
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 26.291675057143546, agent episode reward: [9.956730136669398, 9.766056029790517, 9.761605286919247, 9.767176253393558, -3.282258785522311, -9.67763386410686], time: 318.29
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 27.589239659698073, agent episode reward: [10.686875507579488, 10.542235759331799, 10.489032624070688, 10.498461644127499, -3.596076316195883, -11.031289559215521], time: 314.687
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 26.237710009175462, agent episode reward: [10.507633868859566, 10.307158731478717, 10.325637613875628, 10.354735165479356, -3.933765971209893, -11.323689399307911], time: 315.367
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 28.46437784418439, agent episode reward: [10.246438587197636, 10.038413179492542, 10.042335900134146, 10.09368065031289, -4.51008129902374, -7.446409173929087], time: 315.787
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 29.90696952709815, agent episode reward: [10.51199608507388, 10.408324284012657, 10.482301713470996, 10.454444934318788, -3.7104978981786436, -8.239599591599525], time: 318.782
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 28.12565107100248, agent episode reward: [9.70026603108087, 9.62814430866134, 9.737212962936509, 9.846407637167907, -3.5847636473542623, -7.201616221489883], time: 316.88
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 29.123557592275315, agent episode reward: [10.416091725809114, 10.069470912200941, 10.14648982918204, 10.288497058435784, -2.9301749704523155, -8.866816962900245], time: 319.03
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 29.23017264261619, agent episode reward: [10.044014732397274, 9.85180232647406, 9.966171658869586, 10.059049167526812, -2.85906512576686, -7.831800116884683], time: 316.497
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 30.75285687736974, agent episode reward: [10.433773374741294, 10.263098050950871, 10.280786141842567, 10.41776504308957, -2.4155130272023153, -8.227052706052246], time: 316.465
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 31.129010766266198, agent episode reward: [10.664470508285921, 10.447677733591812, 10.53422986421803, 10.612748501607722, -2.4726760668742944, -8.657439774562999], time: 316.505
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 28.323594698331334, agent episode reward: [9.861404279095868, 9.593273074215023, 9.7626907410363, 9.802370220808559, -2.270236292268614, -8.425907324555807], time: 315.06
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 31.440323243225453, agent episode reward: [10.609425088355543, 10.516623613772781, 10.631123982199547, 10.559936482043307, -2.1433618319619283, -8.7334240911838], time: 316.428
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 29.001893771687275, agent episode reward: [10.022341129714993, 9.86499427164023, 9.86925009637858, 9.894610980723595, -2.469259435823984, -8.18004327094614], time: 314.747
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 31.181110763821284, agent episode reward: [10.731113914987805, 10.532438110169641, 10.494598928508141, 10.692794292663134, -2.2757983300131186, -8.994036152494315], time: 314.854
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 28.253435291239438, agent episode reward: [9.606051491056586, 9.420717101606089, 9.432879343468628, 9.585848036048265, -2.5956734429810973, -7.196387237959032], time: 313.696
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 32.98459166616651, agent episode reward: [11.15738774610814, 10.912341727188586, 10.953791576435705, 11.056581752872951, -2.2532047606750942, -8.842306375763783], time: 312.119
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 29.631890786610963, agent episode reward: [10.252004316642873, 10.105198946822776, 10.060023916324715, 10.240894399051841, -2.5756377026276276, -8.450593089603617], time: 311.573
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 28.63472303382925, agent episode reward: [10.354789407975954, 10.211644218974106, 10.119206831617605, 10.374902176632638, -2.518640432602389, -9.907179168768668], time: 316.031
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 30.451197820956864, agent episode reward: [10.517532891067626, 10.317418671179633, 10.280253191494019, 10.425322490262193, -2.3862286994533233, -8.703100723593279], time: 314.815
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 30.664696798055306, agent episode reward: [10.691642898161936, 10.61685911692032, 10.492989051526152, 10.619024086096134, -2.8247704599934442, -8.931047894655794], time: 313.476
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 32.93777074712053, agent episode reward: [11.321734140438005, 11.036814957357882, 10.888018413539749, 11.010739787937785, -2.994031057849579, -8.325505494303314], time: 284.004
...Finished total of 60001 episodes.
