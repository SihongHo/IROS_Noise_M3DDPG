0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -23.487900810597356, agent episode reward: [-23.806887215369265, 0.15949320238595588, 0.15949320238595588], time: 86.292
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -23.43547381153501, agent episode reward: [-19.458563562173406, -1.9884551246808, -1.9884551246808], time: 112.072
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -18.5888556783366, agent episode reward: [-14.55305523965392, -2.01790021934134, -2.01790021934134], time: 111.53
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -10.864594625039697, agent episode reward: [-11.803508710746723, 0.46945704285351253, 0.46945704285351253], time: 111.506
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -8.941090296090863, agent episode reward: [-12.535206350144485, 1.7970580270268106, 1.7970580270268106], time: 111.254
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -15.301558296005554, agent episode reward: [-16.894202036618914, 0.7963218703066789, 0.7963218703066789], time: 110.932
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -14.547902901828241, agent episode reward: [-15.85543566583296, 0.6537663820023598, 0.6537663820023598], time: 111.731
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -11.848718802193655, agent episode reward: [-16.11847592100979, 2.134878559408068, 2.134878559408068], time: 112.294
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -20.32527225711033, agent episode reward: [-29.451191460459743, 4.562959601674708, 4.562959601674708], time: 111.898
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.753459910963786, agent episode reward: [-34.51050334056473, 9.878521714800474, 9.878521714800474], time: 112.388
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -23.12327097711579, agent episode reward: [-24.218673315282505, 0.5477011690833546, 0.5477011690833546], time: 112.054
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -11.252390675414008, agent episode reward: [-22.72387567485169, 5.7357424997188415, 5.7357424997188415], time: 112.399
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 4.973280932591524, agent episode reward: [-22.670034453861366, 13.821657693226445, 13.821657693226445], time: 112.467
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 19.311728669370588, agent episode reward: [-22.75629640637977, 21.034012537875178, 21.034012537875178], time: 112.326
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 19.48112648116816, agent episode reward: [-22.83834403136489, 21.159735256266526, 21.159735256266526], time: 112.465
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 19.404723835854057, agent episode reward: [-23.042068924009953, 21.22339637993201, 21.22339637993201], time: 112.273
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 18.864952364607394, agent episode reward: [-22.846667221550753, 20.855809793079075, 20.855809793079075], time: 111.692
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 16.568727449281308, agent episode reward: [-21.660510104295565, 19.11461877678844, 19.11461877678844], time: 111.817
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 16.837739334295534, agent episode reward: [-22.215685512562843, 19.52671242342919, 19.52671242342919], time: 112.521
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 16.105501455821642, agent episode reward: [-20.721738022253817, 18.413619739037728, 18.413619739037728], time: 111.728
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 17.067648397387238, agent episode reward: [-21.228130218036945, 19.147889307712088, 19.147889307712088], time: 112.542
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 17.014183452676672, agent episode reward: [-21.46547526059539, 19.23982935663603, 19.23982935663603], time: 112.643
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 16.189184336218027, agent episode reward: [-20.10933947085806, 18.14926190353804, 18.14926190353804], time: 112.371
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 16.74666592790781, agent episode reward: [-20.791778303989922, 18.769222115948864, 18.769222115948864], time: 112.807
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 16.959558227532604, agent episode reward: [-20.895606811278697, 18.927582519405654, 18.927582519405654], time: 112.385
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 15.923791179798762, agent episode reward: [-20.0785064340562, 18.001148806927482, 18.001148806927482], time: 112.771
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 15.402241452368926, agent episode reward: [-20.025905148820645, 17.714073300594784, 17.714073300594784], time: 113.105
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 14.832580236314593, agent episode reward: [-18.82557995879497, 16.829080097554783, 16.829080097554783], time: 111.933
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 14.726040333736158, agent episode reward: [-18.859113241911885, 16.79257678782402, 16.79257678782402], time: 112.0
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 16.44284776583342, agent episode reward: [-20.00073606391413, 18.221791914873776, 18.221791914873776], time: 112.431
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 15.397336718214717, agent episode reward: [-19.01142963742995, 17.204383177822333, 17.204383177822333], time: 112.024
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 15.514820827473352, agent episode reward: [-19.016694279393267, 17.26575755343331, 17.26575755343331], time: 112.457
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 13.619848780788404, agent episode reward: [-17.485817351239092, 15.552833066013749, 15.552833066013749], time: 112.759
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 15.867299556655796, agent episode reward: [-19.165858408006986, 17.51657898233139, 17.51657898233139], time: 112.109
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 15.975087586488725, agent episode reward: [-20.361151465642116, 18.16811952606542, 18.16811952606542], time: 112.173
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 14.714967085974678, agent episode reward: [-18.940847830829522, 16.827907458402102, 16.827907458402102], time: 112.977
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 13.72169374639307, agent episode reward: [-17.444572795145305, 15.583133270769187, 15.583133270769187], time: 112.192
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 13.283433513675352, agent episode reward: [-17.45896519215467, 15.371199352915012, 15.371199352915012], time: 112.518
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 14.03786765994987, agent episode reward: [-17.86288876548316, 15.950378212716513, 15.950378212716513], time: 112.949
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 12.669462184092488, agent episode reward: [-16.46437453771524, 14.566918360903864, 14.566918360903864], time: 112.359
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 12.13128131966438, agent episode reward: [-15.933449488632142, 14.032365404148262, 14.032365404148262], time: 112.376
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 12.909227460631366, agent episode reward: [-17.473807508988415, 15.191517484809891, 15.191517484809891], time: 112.934
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 9.598210928291682, agent episode reward: [-18.67677331358217, 14.137492120936924, 14.137492120936924], time: 112.071
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 12.786415624981446, agent episode reward: [-19.45427081892471, 16.12034322195308, 16.12034322195308], time: 112.587
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 7.3235378844210395, agent episode reward: [-20.908007751479367, 14.115772817950202, 14.115772817950202], time: 112.32
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 19.875267075340155, agent episode reward: [-24.57330619607321, 22.224286635706687, 22.224286635706687], time: 113.425
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 17.780919088154157, agent episode reward: [-21.679174047757623, 19.730046567955892, 19.730046567955892], time: 112.025
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 18.393587773436003, agent episode reward: [-22.011289763542997, 20.202438768489504, 20.202438768489504], time: 113.143
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 18.335386629146274, agent episode reward: [-21.969978987525455, 20.152682808335864, 20.152682808335864], time: 112.828
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 16.159284682169158, agent episode reward: [-22.261884080468967, 19.21058438131906, 19.21058438131906], time: 112.637
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 18.06195494203264, agent episode reward: [-26.05652587468092, 22.05924040835678, 22.05924040835678], time: 113.208
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 19.553332943842932, agent episode reward: [-26.254065239833093, 22.903699091838014, 22.903699091838014], time: 113.092
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 6.436505486609276, agent episode reward: [-23.162842690518826, 14.799674088564046, 14.799674088564046], time: 113.626
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -6.240474515939832, agent episode reward: [-18.98640010814057, 6.372962796100372, 6.372962796100372], time: 112.908
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -8.978183437553627, agent episode reward: [-18.913952963717563, 4.967884763081967, 4.967884763081967], time: 113.074
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -4.835749086497155, agent episode reward: [-23.32583047215585, 9.245040692829347, 9.245040692829347], time: 113.048
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -18.522747489553044, agent episode reward: [-22.691257869467503, 2.0842551899572284, 2.0842551899572284], time: 112.856
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -6.045852634484674, agent episode reward: [-21.04797221666531, 7.501059791090319, 7.501059791090319], time: 112.897
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -6.468604817779418, agent episode reward: [-20.196118558972557, 6.8637568705965695, 6.8637568705965695], time: 113.005
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -9.687207314612449, agent episode reward: [-16.55529387544778, 3.4340432804176655, 3.4340432804176655], time: 94.892
...Finished total of 60001 episodes.
