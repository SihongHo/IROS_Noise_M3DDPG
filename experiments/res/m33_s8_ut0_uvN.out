0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
3 bad agents
      adv rate for q_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
4 good agents
      adv rate for q_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
5 good agents
      adv rate for q_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 4 adversaries
Uncertainty type is:  None ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.984803605033736, agent episode reward: [1.0522292004568323, 1.0217173636055459, 1.0534752086662698, 1.0379658079224732, -11.697733691771083, -18.452457493913776], time: 218.782
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -14.051576128450312, agent episode reward: [2.6391623836023523, 2.8254882081139314, 2.7042360208777416, 3.2199890929126016, -8.297103543505772, -17.143348290451165], time: 277.285
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 13.026627541467471, agent episode reward: [4.13102087233171, 4.232459056611805, 4.272958820643745, 4.61215371920635, -2.1276899187301668, -2.0942750085959747], time: 275.373
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 20.749972908756927, agent episode reward: [6.773553900528832, 6.728861920417625, 6.932065101818298, 6.600274065382946, -3.2685321797400055, -3.0162498996507683], time: 274.516
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 30.2689741550987, agent episode reward: [9.970694314341104, 9.903715676717558, 10.042685649355949, 9.701444062761187, -5.297542821269977, -4.052022726807125], time: 273.734
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 33.01380643711814, agent episode reward: [10.794117089285544, 10.688121312368013, 10.849457406256487, 10.548236872553549, -4.290329539078581, -5.5757967042668755], time: 274.456
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 35.307927164712844, agent episode reward: [11.562808063383956, 11.488907611166168, 11.707089758732513, 11.331219850876176, -3.659906971799478, -7.122191147646496], time: 275.605
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 32.37713509749861, agent episode reward: [10.718045260043862, 10.62186812644808, 10.834869957000658, 10.450932629745083, -2.786566088761154, -7.462014786977922], time: 273.155
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 34.284903959702184, agent episode reward: [11.646179016245705, 11.3825774774381, 11.602491932141255, 11.306858638039655, -4.194447097922301, -7.458756006240231], time: 275.294
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 45.6463440945499, agent episode reward: [15.282424231395936, 15.213702363935608, 15.289465143974448, 15.064478680184488, -5.831095230673349, -9.372631094267232], time: 273.524
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 43.65462135991851, agent episode reward: [14.535520423029514, 14.659752543670404, 14.596517531676389, 14.461792710715748, -4.8462863013492985, -9.752675547824248], time: 273.461
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 44.802030058683336, agent episode reward: [15.006403207910662, 15.077194883551657, 15.032742391197052, 14.977085830452237, -5.643118061557885, -9.648278192870391], time: 275.437
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 37.55390908431394, agent episode reward: [12.366106056231988, 12.432018419916222, 12.378312490517681, 12.351988462166416, -4.0364181713693705, -7.938098173148999], time: 275.523
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 33.59892267645561, agent episode reward: [11.272271582896698, 11.308723027736141, 11.17850473486898, 11.263589887608108, -3.7604637155060656, -7.663702841148246], time: 275.953
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 28.17527461042181, agent episode reward: [9.65230471300969, 9.630081807013587, 9.547805059654005, 9.553446232180653, -3.480217761331828, -6.7281454401042975], time: 273.724
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 20.21926484143915, agent episode reward: [6.844635558980061, 6.806491499754587, 6.808588120763096, 6.719962471362889, -1.8917623122160852, -5.068650497205399], time: 273.271
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 16.40140718735537, agent episode reward: [5.566086544114805, 5.520505022345192, 5.598421892470865, 5.482510362037879, -1.9254874361237433, -3.8406291974896303], time: 275.478
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 17.116746116108875, agent episode reward: [5.755534231133374, 5.662067823306494, 5.7489769215873405, 5.650833563036751, -1.575641755195304, -4.125024667759777], time: 276.37
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 16.272616912539398, agent episode reward: [5.536059275343044, 5.404381822171668, 5.478474974569495, 5.440519976120908, -1.2358725141541491, -4.350946621511569], time: 273.552
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 18.94804061163702, agent episode reward: [6.4581137016390615, 6.2799832845608865, 6.3649316993937575, 6.346425865835571, -1.1822360884127951, -5.319177851379462], time: 274.126
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 19.140779090170266, agent episode reward: [6.5621718882997175, 6.34646025936038, 6.357886687030047, 6.3833758026459275, -1.159936248163253, -5.349179299002554], time: 273.641
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 22.729390819760216, agent episode reward: [7.76437963605329, 7.648625651597129, 7.652688134282393, 7.636176581519143, -1.6985380593419945, -6.27394112434974], time: 274.069
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 20.546270104032736, agent episode reward: [7.182505863711992, 7.043587480375398, 7.042625161969972, 7.00794054151019, -1.9135635855645776, -5.816825357970238], time: 273.767
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 18.58136960557888, agent episode reward: [6.485048076062803, 6.384779649224727, 6.34967997924691, 6.273673189867997, -2.0034992831261005, -4.908312005697456], time: 273.267
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 19.861179025505653, agent episode reward: [6.784077241375935, 6.666188464520773, 6.589650328522243, 6.628030631982052, -1.5323601719314792, -5.27440746896387], time: 273.212
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 19.73317922742312, agent episode reward: [6.740386692218417, 6.628452219521115, 6.542451408502087, 6.545352660906934, -1.6839371142114095, -5.039526639514028], time: 275.316
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 23.3646020072566, agent episode reward: [7.906957176198804, 7.766828611361949, 7.706655236495308, 7.620939857974888, -1.8625546763971363, -5.774224198377215], time: 273.848
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 23.327732759125798, agent episode reward: [7.845117410795759, 7.710334833893785, 7.611682695720774, 7.596474848276024, -1.7037205364337151, -5.73215649312683], time: 272.135
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 22.843977554268815, agent episode reward: [7.950949701031351, 7.812142983965052, 7.76494614204536, 7.711736956485899, -1.9689130399077266, -6.42688518935112], time: 269.659
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 25.19643943140289, agent episode reward: [8.894186410588041, 8.715976096406605, 8.674150035995895, 8.656351460596325, -2.2887422261933192, -7.455482345990655], time: 270.235
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 28.211681562801328, agent episode reward: [9.706884238583772, 9.559664476276915, 9.513673679692653, 9.429020578159122, -2.409375852132899, -7.588185557778231], time: 268.088
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 27.051664577531614, agent episode reward: [9.203926397431097, 9.062838758304473, 9.024825539714625, 8.947161813740799, -2.0898033367670625, -7.097284594892317], time: 269.717
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 31.863061632045557, agent episode reward: [10.839466150773584, 10.705321615152183, 10.651767093279656, 10.562871077694227, -2.4307992850685434, -8.465565019785554], time: 266.307
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 31.544396047310453, agent episode reward: [10.929722600529617, 10.728312613571198, 10.67061976784162, 10.52527460318007, -2.557975194430505, -8.751558343381546], time: 266.396
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 40.31218513083656, agent episode reward: [13.678650477734655, 13.439658954720853, 13.449266618484952, 13.190340954741565, -1.7623542211688292, -11.68337765367663], time: 266.753
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 41.832530692952105, agent episode reward: [14.104114052234543, 13.82742005621154, 13.88121479628866, 13.51992858609115, -1.6842998946291505, -11.815846903244644], time: 265.828
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 38.55689028926748, agent episode reward: [12.92964438676053, 12.72614783404374, 12.884019440854367, 12.382414434319733, -1.6052527146963138, -10.76008309201458], time: 267.426
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 34.95182288390723, agent episode reward: [11.728357023571451, 11.604219552104421, 11.646339109218388, 11.12618676295458, -1.5617203966536677, -9.591559167287956], time: 264.443
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 32.357537222401824, agent episode reward: [10.904326721463972, 10.844373796432548, 10.879392180766757, 10.547826766715913, -1.5946370447181863, -9.22374519825919], time: 266.867
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 40.22079542422778, agent episode reward: [13.68495084829097, 13.390014459056504, 13.453143488151621, 13.237314457344075, -1.5291995509263347, -12.015428277689058], time: 264.359
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 44.35020952405968, agent episode reward: [15.162089390826692, 14.710960841155627, 14.852611693096897, 14.669857343703027, -1.2865404192110514, -13.758769325511516], time: 263.832
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 41.76287683724038, agent episode reward: [14.271042740552618, 13.772562001401988, 13.904448699628269, 13.85103567286758, -1.206624647875808, -12.829587629334263], time: 263.483
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 48.795535170846684, agent episode reward: [16.586832405814203, 16.057780891931944, 16.15238400712794, 16.179331119909765, -1.161770954607909, -15.019022299329256], time: 263.445
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 44.83866258603241, agent episode reward: [15.449498126336218, 14.900411962984418, 15.050492097033256, 15.043603078932792, -1.4167408169608746, -14.188601862293401], time: 261.564
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 40.76723701358313, agent episode reward: [14.229435368836853, 13.753505243229979, 13.809200646577938, 13.784460315728277, -1.870290589244632, -12.939073971545287], time: 263.451
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 37.91620890399413, agent episode reward: [13.274004586483867, 12.893626451815557, 12.831960097275228, 12.812972592512866, -1.8319966187174819, -12.064358205375905], time: 264.317
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 35.977902815778734, agent episode reward: [12.695243539988073, 12.26576342302316, 12.3626116157277, 12.241244569180893, -2.2901034429607443, -11.29685688918035], time: 265.519
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 35.69969263857938, agent episode reward: [12.598526719582312, 12.06256985568627, 12.326079853187892, 12.14408029966581, -2.433661658777326, -10.997902430765578], time: 266.265
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 34.201603344265926, agent episode reward: [11.920213021856398, 11.41132235002843, 11.635287773032493, 11.499571132332148, -1.7905496161126833, -10.47424131687087], time: 262.121
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 34.79693997492471, agent episode reward: [11.946029190824698, 11.552257346257315, 11.713874408180109, 11.545978754449967, -2.0893242166838517, -9.871875508103532], time: 261.067
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 35.97811899925211, agent episode reward: [12.266073351734828, 11.76866229416814, 11.987400585158552, 11.849427388857148, -1.1395954542877516, -10.753849166378801], time: 263.586
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 30.959893078402747, agent episode reward: [10.871316412508204, 10.384138243702358, 10.609107945035328, 10.462303850647167, -2.086020564293798, -9.280952809196512], time: 260.945
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 32.71132995659988, agent episode reward: [11.409010252893397, 10.967042165725081, 11.10945921013335, 11.070212064351464, -2.1614405138392496, -9.682953222664157], time: 262.127
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 29.335196527778454, agent episode reward: [10.592325662345417, 10.103342840685338, 10.299422513011988, 10.2103282527435, -2.141848630544324, -9.728374110463458], time: 262.951
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 26.657198907010024, agent episode reward: [9.657649800012077, 9.20624952883072, 9.414381297006253, 9.306720640339266, -2.1024384810491505, -8.825363878129144], time: 264.393
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 31.402837483628144, agent episode reward: [11.128865869374524, 10.833600926070911, 10.91105223302828, 10.821606897110204, -2.2753009548457057, -10.01698748711007], time: 263.313
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 26.921355834126604, agent episode reward: [9.941827917217076, 9.635557872116118, 9.687196108342068, 9.550268785200846, -2.5224747461579176, -9.371020102591581], time: 263.749
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 24.421936963064955, agent episode reward: [9.103288628034358, 8.814544279101236, 8.782007261605566, 8.750896852740254, -2.3334749885178665, -8.695325069898592], time: 263.957
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 30.019219436116167, agent episode reward: [11.082661175229113, 10.826311562836047, 10.800248788082117, 10.75571517385968, -3.310557853525853, -10.135159410364937], time: 226.392
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 30.092531252587317, agent episode reward: [11.040774092357015, 10.76503505441257, 10.755136814413211, 10.711992782631384, -3.122393095323627, -10.058014395903234], time: 201.779
...Finished total of 60001 episodes.
