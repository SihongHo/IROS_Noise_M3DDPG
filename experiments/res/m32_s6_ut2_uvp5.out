0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -24.508076964301008, agent episode reward: [-24.715240767760747, 0.10358190172987372, 0.10358190172987372], time: 68.369
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -13.180888553130153, agent episode reward: [-9.65884353803869, -1.761022507545732, -1.761022507545732], time: 110.224
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -3.79568450262023, agent episode reward: [-2.093790198377592, -0.8509471521213188, -0.8509471521213188], time: 112.471
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -9.161076505247939, agent episode reward: [-9.518855612162938, 0.17888955345750035, 0.17888955345750035], time: 111.908
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.54350406715403, agent episode reward: [-13.62199134863224, 0.03924364073910557, 0.03924364073910557], time: 112.406
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -17.89585536142612, agent episode reward: [-19.935800757205026, 1.0199726978894563, 1.0199726978894563], time: 111.912
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -21.61343685378867, agent episode reward: [-33.44183414344382, 5.914198644827576, 5.914198644827576], time: 112.697
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 9.219113512786548, agent episode reward: [-31.243652925312766, 20.231383219049654, 20.231383219049654], time: 112.598
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 19.328958624401245, agent episode reward: [-22.55552888986228, 20.942243757131763, 20.942243757131763], time: 113.247
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 9.87552943613723, agent episode reward: [-21.338505823924475, 15.607017630030851, 15.607017630030851], time: 111.972
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 9.34236589620929, agent episode reward: [-27.612136287014508, 18.477251091611897, 18.477251091611897], time: 111.822
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 2.8861506204516436, agent episode reward: [-22.696177407754472, 12.791164014103057, 12.791164014103057], time: 112.283
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 7.670681187747108, agent episode reward: [-24.651942074900777, 16.16131163132394, 16.16131163132394], time: 111.08
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 10.89613192536905, agent episode reward: [-23.191209324027273, 17.043670624698162, 17.043670624698162], time: 110.628
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -0.11140474725897911, agent episode reward: [-20.452826228424804, 10.170710740582912, 10.170710740582912], time: 111.977
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 10.564784244096082, agent episode reward: [-17.81370037867603, 14.189242311386057, 14.189242311386057], time: 112.017
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 16.48898141119802, agent episode reward: [-24.111010311947048, 20.299995861572533, 20.299995861572533], time: 111.76
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 20.769331500156976, agent episode reward: [-25.73030538265683, 23.249818441406898, 23.249818441406898], time: 113.062
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 16.34855505096194, agent episode reward: [-20.71254374329286, 18.5305493971274, 18.5305493971274], time: 113.502
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 16.80570087155465, agent episode reward: [-21.337681683479513, 19.07169127751708, 19.07169127751708], time: 112.346
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 15.661508190080335, agent episode reward: [-24.35505398370016, 20.00828108689025, 20.00828108689025], time: 111.888
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 7.787637578651376, agent episode reward: [-22.738614392511654, 15.263125985581514, 15.263125985581514], time: 112.649
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 23.529642040039892, agent episode reward: [-29.41918554147629, 26.47441379075809, 26.47441379075809], time: 113.118
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 21.231254075689613, agent episode reward: [-26.479480897933396, 23.855367486811506, 23.855367486811506], time: 113.174
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 16.28552369946508, agent episode reward: [-20.670894571091573, 18.478209135278323, 18.478209135278323], time: 112.17
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 16.22337383017849, agent episode reward: [-20.644716014613564, 18.434044922396023, 18.434044922396023], time: 111.661
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 16.386608842826934, agent episode reward: [-20.464273983520727, 18.42544141317383, 18.42544141317383], time: 111.921
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 17.06051159262359, agent episode reward: [-21.067356971334434, 19.063934281979016, 19.063934281979016], time: 113.353
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 16.71869134294468, agent episode reward: [-20.95325922304614, 18.83597528299541, 18.83597528299541], time: 112.406
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 16.303325835826026, agent episode reward: [-20.21577816674974, 18.259552001287883, 18.259552001287883], time: 113.242
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 16.269951200051672, agent episode reward: [-20.33468742506783, 18.302319312559753, 18.302319312559753], time: 112.058
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 16.39278255050312, agent episode reward: [-20.585677723594124, 18.48923013704862, 18.48923013704862], time: 112.88
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 16.863195572259485, agent episode reward: [-20.817738629483838, 18.84046710087166, 18.84046710087166], time: 112.188
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 16.58338019524336, agent episode reward: [-20.400361110948573, 18.491870653095965, 18.491870653095965], time: 112.564
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 17.234042322632057, agent episode reward: [-21.296908336251068, 19.265475329441564, 19.265475329441564], time: 111.413
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 16.98623502676476, agent episode reward: [-20.730989886989548, 18.858612456877157, 18.858612456877157], time: 112.057
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 16.514101835792516, agent episode reward: [-20.910507559890785, 18.712304697841653, 18.712304697841653], time: 112.042
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 15.989852223223776, agent episode reward: [-20.027278470129442, 18.00856534667661, 18.00856534667661], time: 113.329
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 15.754984765225071, agent episode reward: [-19.728809134029422, 17.741896949627247, 17.741896949627247], time: 111.818
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 15.961905726306467, agent episode reward: [-19.3961178033072, 17.679011764806834, 17.679011764806834], time: 111.988
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 16.132866079010046, agent episode reward: [-19.594390308674804, 17.863628193842423, 17.863628193842423], time: 113.342
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 14.18924442714768, agent episode reward: [-18.84386008904014, 16.516552258093906, 16.516552258093906], time: 112.728
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 15.76452401795115, agent episode reward: [-20.595245877029193, 18.179884947490173, 18.179884947490173], time: 113.072
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 12.303045420631218, agent episode reward: [-19.27194557020331, 15.787495495417263, 15.787495495417263], time: 112.259
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 23.445270964411254, agent episode reward: [-30.2578482530857, 26.851559608748477, 26.851559608748477], time: 112.519
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 22.82612334241525, agent episode reward: [-28.497814766379292, 25.661969054397268, 25.661969054397268], time: 112.638
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 14.77574699269311, agent episode reward: [-22.375626123718867, 18.575686558205987, 18.575686558205987], time: 113.584
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 14.902857732208707, agent episode reward: [-22.694642424666068, 18.79875007843739, 18.79875007843739], time: 113.854
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 11.006658834266387, agent episode reward: [-23.539631086735962, 17.273144960501178, 17.273144960501178], time: 112.346
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 11.994471607566876, agent episode reward: [-25.3717795708519, 18.68312558920939, 18.68312558920939], time: 113.099
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 9.100675031751372, agent episode reward: [-25.444952285540552, 17.272813658645962, 17.272813658645962], time: 112.598
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 17.095518913881254, agent episode reward: [-20.969256457467576, 19.03238768567442, 19.03238768567442], time: 112.974
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 18.597402470052074, agent episode reward: [-21.9428500893855, 20.270126279718788, 20.270126279718788], time: 111.881
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 17.60823975365681, agent episode reward: [-20.94852616366301, 19.27838295865991, 19.27838295865991], time: 111.427
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 18.427205923526948, agent episode reward: [-21.600685784967688, 20.013945854247318, 20.013945854247318], time: 113.107
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 18.386306575270414, agent episode reward: [-21.5100771731188, 19.948191874194606, 19.948191874194606], time: 113.776
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 17.655818540539258, agent episode reward: [-21.1190079891894, 19.38741326486433, 19.38741326486433], time: 113.327
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 19.06636696918705, agent episode reward: [-22.56016974297679, 20.81326835608192, 20.81326835608192], time: 113.487
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 19.545992133824914, agent episode reward: [-34.598955982809656, 27.072474058317283, 27.072474058317283], time: 114.042
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 31.147312797689, agent episode reward: [-34.063602426935624, 32.605457612312314, 32.605457612312314], time: 109.476
...Finished total of 60001 episodes.
