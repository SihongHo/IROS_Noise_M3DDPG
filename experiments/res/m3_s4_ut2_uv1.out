0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -22.630885743069296, agent episode reward: [-37.11433249067427, 7.24172337380249, 7.24172337380249], time: 45.429
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -25.695633378194273, agent episode reward: [-33.35322654981536, 3.8287965858105397, 3.8287965858105397], time: 81.962
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -5.3021339458800565, agent episode reward: [-13.12504585151028, 3.9114559528151123, 3.9114559528151123], time: 82.501
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 2.438799297639883, agent episode reward: [-11.48229329786407, 6.960546297751976, 6.960546297751976], time: 84.077
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 3.5511543347048113, agent episode reward: [-9.895099428536529, 6.72312688162067, 6.72312688162067], time: 83.354
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 3.116342677773216, agent episode reward: [-9.701678342509918, 6.4090105101415675, 6.4090105101415675], time: 83.486
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 3.6454210703660226, agent episode reward: [-10.022084210406684, 6.833752640386352, 6.833752640386352], time: 84.267
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 4.431210103580058, agent episode reward: [-10.312390855798045, 7.371800479689051, 7.371800479689051], time: 83.289
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 4.007364596476008, agent episode reward: [-11.426921740735759, 7.717143168605884, 7.717143168605884], time: 83.879
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 2.7849586331155805, agent episode reward: [-10.982874197989, 6.8839164155522905, 6.8839164155522905], time: 84.558
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 1.9492731385401694, agent episode reward: [-10.56480515131589, 6.257039144928029, 6.257039144928029], time: 83.584
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 2.2060614747640366, agent episode reward: [-11.914493671034183, 7.06027757289911, 7.06027757289911], time: 84.558
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 0.31767582478339523, agent episode reward: [-11.753537081571993, 6.035606453177694, 6.035606453177694], time: 85.115
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 0.8842722408037882, agent episode reward: [-12.160618758386198, 6.522445499594992, 6.522445499594992], time: 86.317
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 0.9619705672219586, agent episode reward: [-13.566953979029742, 7.264462273125851, 7.264462273125851], time: 84.265
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 1.6261528149726627, agent episode reward: [-13.177187060618413, 7.401669937795539, 7.401669937795539], time: 83.756
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 1.153690833765519, agent episode reward: [-12.77828767167329, 6.965989252719404, 6.965989252719404], time: 83.49
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 0.6887817840120658, agent episode reward: [-11.248318725885206, 5.968550254948635, 5.968550254948635], time: 83.741
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 1.361888063219132, agent episode reward: [-12.410716013908829, 6.88630203856398, 6.88630203856398], time: 84.264
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 1.6489567621501267, agent episode reward: [-12.319628480899333, 6.984292621524729, 6.984292621524729], time: 84.278
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 1.130610351429913, agent episode reward: [-12.71499064361, 6.922800497519956, 6.922800497519956], time: 84.698
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 1.4313117633526462, agent episode reward: [-13.521501343284262, 7.476406553318455, 7.476406553318455], time: 84.614
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 1.3715582545889522, agent episode reward: [-14.542410627385763, 7.956984440987359, 7.956984440987359], time: 83.918
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 1.8122698704562459, agent episode reward: [-13.19899535644972, 7.505632613452983, 7.505632613452983], time: 84.555
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 1.0381287030300719, agent episode reward: [-12.346582372757712, 6.692355537893892, 6.692355537893892], time: 83.566
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 1.343438396262372, agent episode reward: [-13.629570190209643, 7.486504293236007, 7.486504293236007], time: 83.618
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 1.3693661454952377, agent episode reward: [-13.145434029556524, 7.25740008752588, 7.25740008752588], time: 84.092
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 1.3681404960311057, agent episode reward: [-13.08036932296468, 7.224254909497893, 7.224254909497893], time: 84.138
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 0.5123339941889892, agent episode reward: [-11.711451610297662, 6.111892802243325, 6.111892802243325], time: 84.655
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 1.2399985303153576, agent episode reward: [-12.77428534179994, 7.00714193605765, 7.00714193605765], time: 85.023
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 1.9702697884043205, agent episode reward: [-12.967729090799821, 7.468999439602072, 7.468999439602072], time: 83.862
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 1.1480464607160434, agent episode reward: [-13.446346792170527, 7.297196626443285, 7.297196626443285], time: 84.693
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 1.0209004267103348, agent episode reward: [-13.185518636281117, 7.103209531495726, 7.103209531495726], time: 85.721
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -0.08271799136230452, agent episode reward: [-12.627441863620035, 6.272361936128866, 6.272361936128866], time: 85.01
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 0.5459870765983335, agent episode reward: [-11.92702456764287, 6.236505822120604, 6.236505822120604], time: 87.025
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 1.1288222620447441, agent episode reward: [-12.801710107251113, 6.965266184647928, 6.965266184647928], time: 88.347
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 1.279923883518847, agent episode reward: [-12.596696040641007, 6.938309962079928, 6.938309962079928], time: 86.575
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 1.008797426723136, agent episode reward: [-12.11091674573156, 6.559857086227347, 6.559857086227347], time: 86.462
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 0.3625631549347281, agent episode reward: [-11.843242722882438, 6.102902938908582, 6.102902938908582], time: 84.666
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 0.3687617487801745, agent episode reward: [-12.45891307464314, 6.4138374117116586, 6.4138374117116586], time: 85.047
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -0.019159178136733856, agent episode reward: [-12.139738344390816, 6.060289583127042, 6.060289583127042], time: 79.845
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 0.22274094687576587, agent episode reward: [-12.115032741573282, 6.168886844224525, 6.168886844224525], time: 80.991
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -0.7790842803671512, agent episode reward: [-11.618792852071536, 5.4198542858521925, 5.4198542858521925], time: 79.984
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -1.0918882038303785, agent episode reward: [-12.270007645216719, 5.589059720693171, 5.589059720693171], time: 82.871
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -0.8200193191319163, agent episode reward: [-12.09128068856027, 5.635630684714177, 5.635630684714177], time: 81.537
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -0.794943704264183, agent episode reward: [-12.252310603974083, 5.72868344985495, 5.72868344985495], time: 78.565
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -0.547645858729609, agent episode reward: [-12.47158263659634, 5.961968388933365, 5.961968388933365], time: 78.736
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -0.1305578736388884, agent episode reward: [-12.178161136311548, 6.02380163133633, 6.02380163133633], time: 80.153
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 0.4526087891366645, agent episode reward: [-12.667021842436462, 6.559815315786563, 6.559815315786563], time: 79.145
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -0.12344660786497667, agent episode reward: [-13.07721088963134, 6.4768821408831805, 6.4768821408831805], time: 79.827
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 0.009428206324097204, agent episode reward: [-13.806862831157508, 6.908145518740803, 6.908145518740803], time: 81.884
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -0.3889313774310553, agent episode reward: [-13.46996418573286, 6.540516404150901, 6.540516404150901], time: 82.627
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 0.7047372276608591, agent episode reward: [-13.437895467371112, 7.071316347515985, 7.071316347515985], time: 79.967
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 0.34644911701302167, agent episode reward: [-13.505589345076652, 6.926019231044837, 6.926019231044837], time: 78.05
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 1.0422221365688993, agent episode reward: [-12.40939133129784, 6.7258067339333705, 6.7258067339333705], time: 79.543
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 0.3223912506560027, agent episode reward: [-13.012640908396971, 6.667516079526488, 6.667516079526488], time: 80.763
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -0.4402087378596154, agent episode reward: [-12.729033481359876, 6.144412371750132, 6.144412371750132], time: 79.152
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -0.26828841406067405, agent episode reward: [-13.563424002055259, 6.647567793997293, 6.647567793997293], time: 75.981
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -0.5955230918985751, agent episode reward: [-13.13216683274099, 6.268321870421207, 6.268321870421207], time: 75.723
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 0.04582230438348279, agent episode reward: [-12.89810263351096, 6.471962468947221, 6.471962468947221], time: 73.991
...Finished total of 60001 episodes.
