0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -2.148760503040602, agent episode reward: [2.5, 2.5, 2.5, -9.6487605030406], time: 114.546
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -1.2637854420402155, agent episode reward: [3.8, 3.8, 3.8, -12.663785442040215], time: 158.128
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 4.3363517124033315, agent episode reward: [4.22, 4.22, 4.22, -8.323648287596669], time: 156.072
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 8.714661817395823, agent episode reward: [4.9, 4.9, 4.9, -5.985338182604175], time: 156.706
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 14.048312593896808, agent episode reward: [7.35, 7.35, 7.35, -8.00168740610319], time: 156.179
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 20.20511433740508, agent episode reward: [10.35, 10.35, 10.35, -10.844885662594919], time: 157.065
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 35.92332211132113, agent episode reward: [18.44, 18.44, 18.44, -19.396677888678873], time: 156.183
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 38.52834897049554, agent episode reward: [20.1, 20.1, 20.1, -21.771651029504458], time: 156.518
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 26.24602443162957, agent episode reward: [14.63, 14.63, 14.63, -17.64397556837043], time: 156.582
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 23.520732789186386, agent episode reward: [13.44, 13.44, 13.44, -16.799267210813618], time: 156.098
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 25.228589748337114, agent episode reward: [14.86, 14.86, 14.86, -19.351410251662884], time: 156.961
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 46.98345022205521, agent episode reward: [25.86, 25.86, 25.86, -30.59654977794479], time: 156.644
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 41.018669886774674, agent episode reward: [22.89, 22.89, 22.89, -27.65133011322533], time: 156.765
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 27.47671977443114, agent episode reward: [16.49, 16.49, 16.49, -21.993280225568864], time: 157.023
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 19.942206905930277, agent episode reward: [13.44, 13.44, 13.44, -20.377793094069723], time: 160.495
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 12.62240228870686, agent episode reward: [10.44, 10.44, 10.44, -18.697597711293138], time: 164.124
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 9.834606659890971, agent episode reward: [8.73, 8.73, 8.73, -16.355393340109032], time: 164.511
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 7.324536680737866, agent episode reward: [7.75, 7.75, 7.75, -15.925463319262136], time: 165.861
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 8.604459632400664, agent episode reward: [7.88, 7.88, 7.88, -15.035540367599335], time: 163.406
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 9.812597065886711, agent episode reward: [8.59, 8.59, 8.59, -15.95740293411329], time: 163.842
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 9.173775858849588, agent episode reward: [7.92, 7.92, 7.92, -14.586224141150415], time: 163.519
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 11.737912380392292, agent episode reward: [8.68, 8.68, 8.68, -14.302087619607708], time: 165.071
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 14.487397213587105, agent episode reward: [9.86, 9.86, 9.86, -15.092602786412895], time: 166.812
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 16.538783004290547, agent episode reward: [10.64, 10.64, 10.64, -15.38121699570945], time: 165.139
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 18.07363302587016, agent episode reward: [10.99, 10.99, 10.99, -14.89636697412984], time: 165.694
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 16.401060316863695, agent episode reward: [10.9, 10.9, 10.9, -16.298939683136304], time: 164.871
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 19.31907285143839, agent episode reward: [11.59, 11.59, 11.59, -15.450927148561608], time: 166.015
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 23.607395350278335, agent episode reward: [13.58, 13.58, 13.58, -17.132604649721667], time: 164.92
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 24.765807595001686, agent episode reward: [13.67, 13.67, 13.67, -16.244192404998312], time: 165.93
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 23.944343108600723, agent episode reward: [13.03, 13.03, 13.03, -15.14565689139928], time: 165.211
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 20.665199190727275, agent episode reward: [11.56, 11.56, 11.56, -14.014800809272728], time: 165.668
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 28.693007758022038, agent episode reward: [15.3, 15.3, 15.3, -17.20699224197796], time: 165.808
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 27.895150356402777, agent episode reward: [14.86, 14.86, 14.86, -16.684849643597225], time: 164.327
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 28.459472419865342, agent episode reward: [15.21, 15.21, 15.21, -17.170527580134657], time: 164.296
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 28.41078806731491, agent episode reward: [15.25, 15.25, 15.25, -17.33921193268509], time: 165.498
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 26.13909773126958, agent episode reward: [14.13, 14.13, 14.13, -16.25090226873042], time: 165.788
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 25.730949066908295, agent episode reward: [13.99, 13.99, 13.99, -16.239050933091704], time: 165.024
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 21.46394426670664, agent episode reward: [11.86, 11.86, 11.86, -14.11605573329336], time: 165.636
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 26.175765282201304, agent episode reward: [14.21, 14.21, 14.21, -16.454234717798695], time: 165.691
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 23.612220691460177, agent episode reward: [13.0, 13.0, 13.0, -15.387779308539827], time: 165.047
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 24.71024384590466, agent episode reward: [13.23, 13.23, 13.23, -14.979756154095337], time: 164.837
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 27.276803809600402, agent episode reward: [14.8, 14.8, 14.8, -17.123196190399597], time: 165.226
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 21.75491033657392, agent episode reward: [12.0, 12.0, 12.0, -14.245089663426079], time: 165.012
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 26.75000318947407, agent episode reward: [14.31, 14.31, 14.31, -16.179996810525928], time: 165.646
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 21.62797652200869, agent episode reward: [12.2, 12.2, 12.2, -14.972023477991312], time: 165.794
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 24.258152057746532, agent episode reward: [13.52, 13.52, 13.52, -16.301847942253474], time: 164.233
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 19.941043390530606, agent episode reward: [11.5, 11.5, 11.5, -14.558956609469394], time: 164.519
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 21.23796582507938, agent episode reward: [11.91, 11.91, 11.91, -14.492034174920619], time: 165.551
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 20.41292168202675, agent episode reward: [11.58, 11.58, 11.58, -14.32707831797325], time: 165.672
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 20.447585412061876, agent episode reward: [11.77, 11.77, 11.77, -14.862414587938126], time: 166.003
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 23.105368108000754, agent episode reward: [12.91, 12.91, 12.91, -15.624631891999245], time: 166.05
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 19.807567369743705, agent episode reward: [11.18, 11.18, 11.18, -13.732432630256296], time: 166.105
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 19.7947777976356, agent episode reward: [11.32, 11.32, 11.32, -14.165222202364397], time: 166.726
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 21.69944439949939, agent episode reward: [11.96, 11.96, 11.96, -14.18055560050061], time: 166.037
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 15.264536529836532, agent episode reward: [9.16, 9.16, 9.16, -12.215463470163467], time: 165.314
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 14.451999301602754, agent episode reward: [9.14, 9.14, 9.14, -12.968000698397246], time: 161.459
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 15.352653497473723, agent episode reward: [9.5, 9.5, 9.5, -13.147346502526277], time: 157.184
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 13.852645660396215, agent episode reward: [8.46, 8.46, 8.46, -11.527354339603786], time: 154.048
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 15.28199731126501, agent episode reward: [9.03, 9.03, 9.03, -11.80800268873499], time: 152.114
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 17.54156557087445, agent episode reward: [10.38, 10.38, 10.38, -13.59843442912555], time: 142.238
...Finished total of 60001 episodes.
