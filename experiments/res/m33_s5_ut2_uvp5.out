0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -1.2748774520336237, agent episode reward: [2.78, 2.78, 2.78, -9.614877452033623], time: 120.005
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -0.9127784043711958, agent episode reward: [3.74, 3.74, 3.74, -12.132778404371198], time: 157.365
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 7.712874417449367, agent episode reward: [4.78, 4.78, 4.78, -6.627125582550632], time: 156.118
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 11.854228031096278, agent episode reward: [6.41, 6.41, 6.41, -7.375771968903722], time: 157.1
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 15.168048020171405, agent episode reward: [7.96, 7.96, 7.96, -8.711951979828592], time: 156.598
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 36.07242373920196, agent episode reward: [18.31, 18.31, 18.31, -18.857576260798044], time: 156.996
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 61.48130663732426, agent episode reward: [31.33, 31.33, 31.33, -32.508693362675736], time: 157.687
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 44.6023604899182, agent episode reward: [24.86, 24.86, 24.86, -29.9776395100818], time: 156.872
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 33.06266408502501, agent episode reward: [21.28, 21.28, 21.28, -30.777335914974994], time: 157.713
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 39.593617153833456, agent episode reward: [23.41, 23.41, 23.41, -30.636382846166548], time: 156.578
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 36.91754180642779, agent episode reward: [21.72, 21.72, 21.72, -28.242458193572208], time: 156.295
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 34.95516912444405, agent episode reward: [20.3, 20.3, 20.3, -25.944830875555958], time: 155.622
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 34.64378350297759, agent episode reward: [19.52, 19.52, 19.52, -23.91621649702241], time: 155.985
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 35.35579758471303, agent episode reward: [20.21, 20.21, 20.21, -25.274202415286968], time: 157.507
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 31.32107291781548, agent episode reward: [18.25, 18.25, 18.25, -23.42892708218452], time: 161.976
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 28.6113270892151, agent episode reward: [17.27, 17.27, 17.27, -23.198672910784904], time: 165.533
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 23.67542943319015, agent episode reward: [14.77, 14.77, 14.77, -20.63457056680985], time: 166.332
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 17.330167575863218, agent episode reward: [11.48, 11.48, 11.48, -17.10983242413678], time: 165.331
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 16.75853563492536, agent episode reward: [10.87, 10.87, 10.87, -15.85146436507464], time: 165.556
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 19.565601238471675, agent episode reward: [11.97, 11.97, 11.97, -16.34439876152833], time: 163.669
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 22.042851768166525, agent episode reward: [13.16, 13.16, 13.16, -17.437148231833476], time: 165.518
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 21.006929197213314, agent episode reward: [12.81, 12.81, 12.81, -17.423070802786686], time: 164.606
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 23.173915732503374, agent episode reward: [14.12, 14.12, 14.12, -19.18608426749662], time: 166.549
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 21.373786094455877, agent episode reward: [13.09, 13.09, 13.09, -17.896213905544126], time: 166.559
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 20.066320963958393, agent episode reward: [12.38, 12.38, 12.38, -17.073679036041604], time: 165.093
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 20.729229432629896, agent episode reward: [12.68, 12.68, 12.68, -17.310770567370106], time: 166.129
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 19.840148182457973, agent episode reward: [12.04, 12.04, 12.04, -16.279851817542024], time: 167.226
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 18.247619630410796, agent episode reward: [11.32, 11.32, 11.32, -15.712380369589203], time: 165.496
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 20.97317855168768, agent episode reward: [12.54, 12.54, 12.54, -16.64682144831232], time: 165.842
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 23.711010688487075, agent episode reward: [13.48, 13.48, 13.48, -16.728989311512922], time: 165.767
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 21.73750635258708, agent episode reward: [12.65, 12.65, 12.65, -16.21249364741292], time: 166.15
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 25.14133055541098, agent episode reward: [14.3, 14.3, 14.3, -17.758669444589017], time: 166.128
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 25.805370272433677, agent episode reward: [14.37, 14.37, 14.37, -17.304629727566326], time: 165.618
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 25.8688474026211, agent episode reward: [14.36, 14.36, 14.36, -17.2111525973789], time: 165.866
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 28.01640678658344, agent episode reward: [15.73, 15.73, 15.73, -19.173593213416563], time: 165.309
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 33.21714725222668, agent episode reward: [18.21, 18.21, 18.21, -21.41285274777332], time: 166.777
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 30.037467226116355, agent episode reward: [16.84, 16.84, 16.84, -20.48253277388364], time: 166.479
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 35.110906772917076, agent episode reward: [19.77, 19.77, 19.77, -24.199093227082926], time: 164.896
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 36.07845621453591, agent episode reward: [20.35, 20.35, 20.35, -24.971543785464092], time: 164.637
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 39.90557093913086, agent episode reward: [22.72, 22.72, 22.72, -28.254429060869136], time: 165.938
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 45.404487459970696, agent episode reward: [25.89, 25.89, 25.89, -32.265512540029306], time: 165.265
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 45.29052412232886, agent episode reward: [25.76, 25.76, 25.76, -31.989475877671133], time: 166.526
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 41.48046561333141, agent episode reward: [24.28, 24.28, 24.28, -31.359534386668585], time: 166.178
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 33.91276198921011, agent episode reward: [20.76, 20.76, 20.76, -28.367238010789887], time: 165.704
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 31.195275693529776, agent episode reward: [19.5, 19.5, 19.5, -27.304724306470224], time: 167.463
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 27.196154312733796, agent episode reward: [17.29, 17.29, 17.29, -24.6738456872662], time: 165.392
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 24.72193079297071, agent episode reward: [16.03, 16.03, 16.03, -23.36806920702929], time: 166.629
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 24.922179016334507, agent episode reward: [15.93, 15.93, 15.93, -22.86782098366549], time: 165.35
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 25.207811785571543, agent episode reward: [16.05, 16.05, 16.05, -22.94218821442846], time: 166.484
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 20.504218037838807, agent episode reward: [14.04, 14.04, 14.04, -21.615781962161194], time: 165.216
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 17.941899757774333, agent episode reward: [12.51, 12.51, 12.51, -19.588100242225668], time: 166.989
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 21.41836256847799, agent episode reward: [13.39, 13.39, 13.39, -18.751637431522013], time: 166.89
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 18.369200831690286, agent episode reward: [12.05, 12.05, 12.05, -17.780799168309713], time: 167.283
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 17.988102125784216, agent episode reward: [12.34, 12.34, 12.34, -19.031897874215787], time: 166.765
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 18.604976030725542, agent episode reward: [12.11, 12.11, 12.11, -17.725023969274456], time: 164.75
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 22.977636907387495, agent episode reward: [14.26, 14.26, 14.26, -19.80236309261251], time: 159.564
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 22.09880931683935, agent episode reward: [13.72, 13.72, 13.72, -19.061190683160646], time: 158.138
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 21.256507840877507, agent episode reward: [13.07, 13.07, 13.07, -17.953492159122494], time: 152.495
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 24.20036855500447, agent episode reward: [14.43, 14.43, 14.43, -19.089631444995533], time: 152.155
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 23.214951089139376, agent episode reward: [13.84, 13.84, 13.84, -18.305048910860624], time: 128.041
...Finished total of 60001 episodes.
