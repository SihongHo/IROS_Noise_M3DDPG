0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.650961537395336, agent episode reward: [-24.163820630899146, -0.7435704532480957, -0.7435704532480957], time: 80.752
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -24.779486799672892, agent episode reward: [-18.005097950761986, -3.3871944244554553, -3.3871944244554553], time: 111.68
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -18.33832491585797, agent episode reward: [-17.825322465175198, -0.25650122534138536, -0.25650122534138536], time: 111.811
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -4.60475042312494, agent episode reward: [-5.424371617228493, 0.4098105970517768, 0.4098105970517768], time: 111.239
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -3.968985612095612, agent episode reward: [-4.4051122171296555, 0.2180633025170218, 0.2180633025170218], time: 112.174
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -12.08961626130626, agent episode reward: [-12.98506314729992, 0.4477234429968293, 0.4477234429968293], time: 112.154
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -13.729128595686682, agent episode reward: [-13.855224704410459, 0.06304805436188828, 0.06304805436188828], time: 113.499
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -18.67136611278158, agent episode reward: [-19.686990488132217, 0.5078121876753204, 0.5078121876753204], time: 112.724
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -13.172153138297453, agent episode reward: [-16.18634299852601, 1.5070949301142769, 1.5070949301142769], time: 113.025
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -9.245051662581618, agent episode reward: [-14.287304894946475, 2.5211266161824293, 2.5211266161824293], time: 112.093
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -20.485386755534957, agent episode reward: [-22.688382015301027, 1.1014976298830357, 1.1014976298830357], time: 112.172
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -19.641146666671506, agent episode reward: [-23.662308649830944, 2.010580991579721, 2.010580991579721], time: 113.0
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -23.377992157042566, agent episode reward: [-32.133855111873, 4.377931477415216, 4.377931477415216], time: 112.497
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -22.972020558191755, agent episode reward: [-29.826969674123603, 3.4274745579659256, 3.4274745579659256], time: 112.169
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 8.232285449096851, agent episode reward: [-20.384268078370184, 14.308276763733518, 14.308276763733518], time: 112.822
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 18.0721041842001, agent episode reward: [-23.652910831085787, 20.862507507642942, 20.862507507642942], time: 112.802
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 19.398391427369916, agent episode reward: [-26.093372115550824, 22.745881771460372, 22.745881771460372], time: 112.459
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 20.387860151488823, agent episode reward: [-26.264266114847263, 23.32606313316804, 23.32606313316804], time: 112.274
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 22.07368144253096, agent episode reward: [-28.174236657837366, 25.123959050184162, 25.123959050184162], time: 113.083
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 13.562123132529493, agent episode reward: [-18.0050056391881, 15.783564385858792, 15.783564385858792], time: 112.909
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 17.42796349048648, agent episode reward: [-20.823601282941613, 19.125782386714047, 19.125782386714047], time: 112.722
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 27.070374735665908, agent episode reward: [-31.015328164006053, 29.04285144983598, 29.04285144983598], time: 112.477
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 21.366041835978503, agent episode reward: [-24.265384754419877, 22.81571329519919, 22.81571329519919], time: 112.529
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 17.781470933449544, agent episode reward: [-21.188904659001135, 19.485187796225343, 19.485187796225343], time: 112.855
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 20.874547309543658, agent episode reward: [-23.801509637644077, 22.33802847359387, 22.33802847359387], time: 112.459
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 20.73580676291592, agent episode reward: [-23.806941214539624, 22.27137398872777, 22.27137398872777], time: 112.402
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 20.904781211749388, agent episode reward: [-24.023805542739545, 22.464293377244466, 22.464293377244466], time: 113.232
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 20.272220821564993, agent episode reward: [-24.73256140014718, 22.502391110856085, 22.502391110856085], time: 112.363
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 19.10744452718021, agent episode reward: [-24.27256281993263, 21.690003673556422, 21.690003673556422], time: 112.692
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 18.378852751664365, agent episode reward: [-23.524600416909227, 20.9517265842868, 20.9517265842868], time: 113.713
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 12.785655922902793, agent episode reward: [-18.731881936883894, 15.758768929893344, 15.758768929893344], time: 112.465
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 21.96085503252587, agent episode reward: [-25.517297594563306, 23.73907631354459, 23.73907631354459], time: 112.552
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 26.496167768554805, agent episode reward: [-29.31370741634519, 27.904937592449997, 27.904937592449997], time: 112.393
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 24.85785407550691, agent episode reward: [-28.201436392033205, 26.529645233770058, 26.529645233770058], time: 112.78
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 16.547296986480323, agent episode reward: [-19.859500508287784, 18.203398747384053, 18.203398747384053], time: 112.175
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 19.97889549543484, agent episode reward: [-23.328836418798428, 21.653865957116636, 21.653865957116636], time: 112.295
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 19.35957085592414, agent episode reward: [-24.066111978143038, 21.71284141703359, 21.71284141703359], time: 111.653
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 16.357107521221028, agent episode reward: [-23.83622723525572, 20.096667378238372, 20.096667378238372], time: 113.343
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 15.031404013084819, agent episode reward: [-23.21279693736222, 19.12210047522352, 19.12210047522352], time: 113.121
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 15.336675749590142, agent episode reward: [-22.220443672497414, 18.77855971104378, 18.77855971104378], time: 112.761
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 17.04389296565191, agent episode reward: [-22.723004085331727, 19.883448525491822, 19.883448525491822], time: 112.7
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 14.87328352365904, agent episode reward: [-21.34898443871279, 18.111133981185915, 18.111133981185915], time: 113.551
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 17.512379752418465, agent episode reward: [-22.25634237489385, 19.884361063656158, 19.884361063656158], time: 113.379
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 17.171841490125754, agent episode reward: [-21.507356307719277, 19.339598898922514, 19.339598898922514], time: 113.231
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 29.60884222685567, agent episode reward: [-32.81511898161169, 31.211980604233677, 31.211980604233677], time: 113.4
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 28.320395527095826, agent episode reward: [-31.742136079409615, 30.031265803252722, 30.031265803252722], time: 113.298
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 29.087715638897826, agent episode reward: [-32.744690269755495, 30.916202954326657, 30.916202954326657], time: 112.95
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 28.797331521677872, agent episode reward: [-32.12632502983644, 30.46182827575715, 30.46182827575715], time: 114.037
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 29.88347325970545, agent episode reward: [-33.107972417742566, 31.49572283872401, 31.49572283872401], time: 112.255
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 29.39412259093319, agent episode reward: [-32.58840103502898, 30.991261812981087, 30.991261812981087], time: 113.197
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 30.03622276446307, agent episode reward: [-33.146924664383654, 31.591573714423358, 31.591573714423358], time: 113.952
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 27.14227813303223, agent episode reward: [-30.354863193958664, 28.748570663495446, 28.748570663495446], time: 112.96
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 22.665797261179893, agent episode reward: [-26.302241594354257, 24.484019427767073, 24.484019427767073], time: 113.291
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 21.60692692745013, agent episode reward: [-25.11635486555843, 23.36164089650428, 23.36164089650428], time: 113.136
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 18.32173777696811, agent episode reward: [-23.199850181255343, 20.760793979111725, 20.760793979111725], time: 112.645
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 11.242021743492733, agent episode reward: [-19.37704540588573, 15.309533574689235, 15.309533574689235], time: 112.543
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 20.071580095876563, agent episode reward: [-23.641677543461608, 21.856628819669083, 21.856628819669083], time: 112.076
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 18.371981072349428, agent episode reward: [-22.82913371055394, 20.600557391451684, 20.600557391451684], time: 112.605
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 17.87103669255457, agent episode reward: [-22.053243960411645, 19.962140326483105, 19.962140326483105], time: 114.312
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 15.338322554995012, agent episode reward: [-23.272916393551977, 19.305619474273495, 19.305619474273495], time: 97.893
...Finished total of 60001 episodes.
