0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.177732579233737, agent episode reward: [-24.374869972813933, -0.401431303209901, -0.401431303209901], time: 77.263
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -24.08412774512742, agent episode reward: [-12.249921939893483, -5.917102902616969, -5.917102902616969], time: 112.295
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -4.832351737096485, agent episode reward: [-3.5369833841755303, -0.6476841764604776, -0.6476841764604776], time: 111.555
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -2.8333755467128303, agent episode reward: [-3.8173792520616465, 0.4920018526744079, 0.4920018526744079], time: 112.073
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -1.2673706212170759, agent episode reward: [-4.519148373650014, 1.6258888762164694, 1.6258888762164694], time: 111.536
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -2.8144785935825136, agent episode reward: [-3.5121241255540734, 0.34882276598578027, 0.34882276598578027], time: 111.51
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -4.206055938395368, agent episode reward: [-3.3200320482631382, -0.4430119450661143, -0.4430119450661143], time: 112.976
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -4.430531366377725, agent episode reward: [-3.800216326096175, -0.3151575201407754, -0.3151575201407754], time: 111.876
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -4.698097120177566, agent episode reward: [-3.4445759471727606, -0.6267605865024025, -0.6267605865024025], time: 113.419
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -4.732410760763467, agent episode reward: [-4.073554405551219, -0.32942817760612386, -0.32942817760612386], time: 112.762
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -5.259651218294831, agent episode reward: [-3.812138895465976, -0.7237561614144276, -0.7237561614144276], time: 112.644
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -5.047840443935024, agent episode reward: [-3.1597958987634445, -0.9440222725857899, -0.9440222725857899], time: 113.139
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -4.382447758561681, agent episode reward: [-3.9314432045367877, -0.22550227701244704, -0.22550227701244704], time: 112.886
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -4.769750273116675, agent episode reward: [-4.114794644639102, -0.3274778142387873, -0.3274778142387873], time: 113.084
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -4.022466983496063, agent episode reward: [-3.6425560369445815, -0.18995547327574058, -0.18995547327574058], time: 113.509
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -3.8514509262066774, agent episode reward: [-3.50869975707076, -0.17137558456795887, -0.17137558456795887], time: 112.812
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -2.9423569918170385, agent episode reward: [-4.0074939768091244, 0.5325684924960431, 0.5325684924960431], time: 112.938
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -1.8794838011683845, agent episode reward: [-5.3685574686890005, 1.744536833760308, 1.744536833760308], time: 112.044
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -3.000801701800952, agent episode reward: [-4.443565752672891, 0.7213820254359693, 0.7213820254359693], time: 112.611
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -4.134769741675354, agent episode reward: [-2.9721605615636206, -0.581304590055867, -0.581304590055867], time: 113.317
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -3.664582783133869, agent episode reward: [-3.7848097498129256, 0.06011348333952802, 0.06011348333952802], time: 113.222
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -3.1295590796969575, agent episode reward: [-5.493756711247741, 1.182098815775392, 1.182098815775392], time: 113.331
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -4.472880109242674, agent episode reward: [-4.452376508743617, -0.010251800249528884, -0.010251800249528884], time: 113.485
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -5.539152881044119, agent episode reward: [-4.658629540473446, -0.4402616702853366, -0.4402616702853366], time: 113.248
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -5.777593082336271, agent episode reward: [-3.3628851774578044, -1.2073539524392336, -1.2073539524392336], time: 112.997
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -2.8343180396967127, agent episode reward: [-4.860757146527741, 1.0132195534155137, 1.0132195534155137], time: 112.647
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -4.638184515778221, agent episode reward: [-5.230546552158667, 0.2961810181902233, 0.2961810181902233], time: 112.259
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -9.093870804589555, agent episode reward: [-10.282513224309518, 0.5943212098599806, 0.5943212098599806], time: 112.501
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -6.7422915354905335, agent episode reward: [-6.526429842366832, -0.10793084656185102, -0.10793084656185102], time: 112.592
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -12.014662290468632, agent episode reward: [-12.586747609829208, 0.2860426596802879, 0.2860426596802879], time: 113.95
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -19.465122853347346, agent episode reward: [-21.04932342180409, 0.7921002842283709, 0.7921002842283709], time: 112.869
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -21.153767827589142, agent episode reward: [-21.940563689061282, 0.3933979307360715, 0.3933979307360715], time: 113.282
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -21.380440714364394, agent episode reward: [-23.475842063296906, 1.0477006744662571, 1.0477006744662571], time: 113.98
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -19.770923258958074, agent episode reward: [-24.189413281736808, 2.2092450113893674, 2.2092450113893674], time: 113.607
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -21.814394289762905, agent episode reward: [-22.659645650286613, 0.4226256802618522, 0.4226256802618522], time: 112.615
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -20.907394244022964, agent episode reward: [-23.100989398837456, 1.0967975774072476, 1.0967975774072476], time: 113.719
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -17.375720170777637, agent episode reward: [-24.314785719853695, 3.4695327745380293, 3.4695327745380293], time: 113.043
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -17.19960723537761, agent episode reward: [-23.666247425594477, 3.2333200951084335, 3.2333200951084335], time: 114.22
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -11.099557815656059, agent episode reward: [-25.065937433951994, 6.983189809147967, 6.983189809147967], time: 113.138
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 0.33573255554412595, agent episode reward: [-29.18561169045982, 14.760672123001973, 14.760672123001973], time: 113.407
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 2.6995292181842125, agent episode reward: [-26.17115040437273, 14.435339811278473, 14.435339811278473], time: 113.277
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 0.6204977119283165, agent episode reward: [-23.5815001501105, 12.100998931019408, 12.100998931019408], time: 113.183
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -3.645843633495166, agent episode reward: [-23.10272960212678, 9.728442984315807, 9.728442984315807], time: 113.918
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 0.12355015035452925, agent episode reward: [-20.763414133029258, 10.443482141691893, 10.443482141691893], time: 113.081
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 4.3892627803779565, agent episode reward: [-21.342626724720198, 12.865944752549076, 12.865944752549076], time: 113.467
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 5.39684471407902, agent episode reward: [-14.561515630006177, 9.9791801720426, 9.9791801720426], time: 113.095
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -4.315892740273675, agent episode reward: [-3.376716449613306, -0.4695881453301846, -0.4695881453301846], time: 113.096
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -5.830508414402037, agent episode reward: [-1.9956511661705831, -1.917428624115727, -1.917428624115727], time: 113.375
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -7.61427088923436, agent episode reward: [-2.6184738984677116, -2.4978984953833243, -2.4978984953833243], time: 112.457
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -7.179904822306701, agent episode reward: [-6.468816024328555, -0.35554439898907264, -0.35554439898907264], time: 113.52
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -7.1123726517864565, agent episode reward: [-16.911928266696776, 4.899777807455161, 4.899777807455161], time: 114.18
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -12.806514276637023, agent episode reward: [-19.771070320943366, 3.4822780221531713, 3.4822780221531713], time: 112.161
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -16.247732662787975, agent episode reward: [-19.098311425863827, 1.425289381537928, 1.425289381537928], time: 113.711
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -18.207281495059245, agent episode reward: [-18.788342897050963, 0.2905307009958618, 0.2905307009958618], time: 113.612
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -20.333982283996335, agent episode reward: [-19.434480825899747, -0.4497507290482933, -0.4497507290482933], time: 113.571
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -18.643662253549817, agent episode reward: [-19.04442377496074, 0.20038076070546434, 0.20038076070546434], time: 113.315
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -19.75986468526484, agent episode reward: [-19.437367511750644, -0.1612485867570994, -0.1612485867570994], time: 112.53
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -19.027849686744077, agent episode reward: [-20.08391569917365, 0.5280330062147848, 0.5280330062147848], time: 112.925
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -17.99438434202539, agent episode reward: [-19.02841162100779, 0.5170136394912028, 0.5170136394912028], time: 113.516
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -15.52576531637973, agent episode reward: [-17.2625664318414, 0.8684005577308355, 0.8684005577308355], time: 99.625
...Finished total of 60001 episodes.
