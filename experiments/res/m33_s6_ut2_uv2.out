0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -23.727258719132756, agent episode reward: [-23.96690707141703, 0.119824176142138, 0.119824176142138], time: 88.089
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -12.960270962793706, agent episode reward: [-9.908813611098934, -1.5257286758473851, -1.5257286758473851], time: 112.729
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -7.705469793476791, agent episode reward: [-10.933374931560332, 1.6139525690417695, 1.6139525690417695], time: 110.928
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -5.910381997310788, agent episode reward: [-2.73863385587158, -1.5858740707196037, -1.5858740707196037], time: 110.968
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -13.107047396401294, agent episode reward: [-6.0805906132448335, -3.513228391578231, -3.513228391578231], time: 110.985
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -12.409654784854434, agent episode reward: [-11.812692449380934, -0.29848116773674976, -0.29848116773674976], time: 111.492
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -7.982763762897162, agent episode reward: [-14.645413646882727, 3.331324941992783, 3.331324941992783], time: 110.988
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -10.585629502313404, agent episode reward: [-12.85486730316927, 1.134618900427933, 1.134618900427933], time: 111.497
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -10.618685256131144, agent episode reward: [-15.355415536330728, 2.3683651400997907, 2.3683651400997907], time: 111.566
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -9.053014041728773, agent episode reward: [-13.141567099297824, 2.044276528784526, 2.044276528784526], time: 112.456
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -9.302380215344943, agent episode reward: [-13.209993004692569, 1.9538063946738122, 1.9538063946738122], time: 112.127
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -11.127558817109216, agent episode reward: [-12.04850621542314, 0.4604736991569629, 0.4604736991569629], time: 112.739
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -9.895967654423533, agent episode reward: [-12.705243550574734, 1.404637948075602, 1.404637948075602], time: 112.152
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -14.904922835150987, agent episode reward: [-16.60902462870536, 0.8520508967771849, 0.8520508967771849], time: 112.264
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -4.318878246813292, agent episode reward: [-25.088702767145048, 10.384912260165876, 10.384912260165876], time: 112.733
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -1.652713724543398, agent episode reward: [-32.320228807915086, 15.333757541685847, 15.333757541685847], time: 112.713
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -0.43001881541983017, agent episode reward: [-29.390340761063513, 14.48016097282184, 14.48016097282184], time: 112.715
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -7.938960725891884, agent episode reward: [-24.626600124672574, 8.343819699390345, 8.343819699390345], time: 112.292
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -5.122150713877394, agent episode reward: [-25.619721273450025, 10.248785279786315, 10.248785279786315], time: 113.27
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -3.8425773289374714, agent episode reward: [-19.61184566015839, 7.884634165610459, 7.884634165610459], time: 111.698
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 6.593578012997659, agent episode reward: [-21.200656496260827, 13.897117254629244, 13.897117254629244], time: 112.656
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 1.0741152395918538, agent episode reward: [-19.9649899264529, 10.519552583022378, 10.519552583022378], time: 112.903
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 2.64546350514907, agent episode reward: [-18.92662073452135, 10.786042119835212, 10.786042119835212], time: 111.986
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 4.3674314257677285, agent episode reward: [-19.192683928432867, 11.7800576771003, 11.7800576771003], time: 111.743
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 2.968659095322364, agent episode reward: [-17.45233866253702, 10.210498878929691, 10.210498878929691], time: 111.275
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 6.574359513310486, agent episode reward: [-18.334541737031184, 12.454450625170837, 12.454450625170837], time: 112.352
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 8.168655470658557, agent episode reward: [-19.170952408829137, 13.669803939743847, 13.669803939743847], time: 111.809
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 9.244231509254323, agent episode reward: [-20.847428172217914, 15.045829840736122, 15.045829840736122], time: 112.924
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 4.940732693469517, agent episode reward: [-18.05097229387157, 11.495852493670547, 11.495852493670547], time: 112.307
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 8.070282111333352, agent episode reward: [-18.7150775154664, 13.392679813399878, 13.392679813399878], time: 112.661
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 7.84705683779058, agent episode reward: [-18.51850448172118, 13.182780659755878, 13.182780659755878], time: 111.658
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 4.737331699558149, agent episode reward: [-18.63061256108586, 11.683972130322005, 11.683972130322005], time: 111.688
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 7.740706858787705, agent episode reward: [-18.50341973122211, 13.12206329500491, 13.12206329500491], time: 112.685
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 7.405464968026324, agent episode reward: [-17.1355475669281, 12.270506267477213, 12.270506267477213], time: 112.233
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 9.057820316246827, agent episode reward: [-19.872875065323544, 14.465347690785187, 14.465347690785187], time: 112.211
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 5.8710580539873, agent episode reward: [-18.36332348144563, 12.117190767716464, 12.117190767716464], time: 112.432
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 14.489453885716763, agent episode reward: [-20.36078610096826, 17.425119993342516, 17.425119993342516], time: 112.836
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 15.265385021574694, agent episode reward: [-20.92473690333646, 18.095060962455577, 18.095060962455577], time: 113.543
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 17.84275981350199, agent episode reward: [-22.059090560636534, 19.95092518706926, 19.95092518706926], time: 113.156
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 16.865194419383645, agent episode reward: [-20.70405715676864, 18.784625788076145, 18.784625788076145], time: 112.384
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 15.832782035687874, agent episode reward: [-21.891529294458234, 18.862155665073058, 18.862155665073058], time: 111.961
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 10.310238735536453, agent episode reward: [-20.922653396778216, 15.616446066157335, 15.616446066157335], time: 112.391
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 15.187906147712724, agent episode reward: [-20.537315998289063, 17.862611073000895, 17.862611073000895], time: 113.073
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 16.379705730574447, agent episode reward: [-21.842869247595395, 19.11128748908492, 19.11128748908492], time: 112.669
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 15.123994063045192, agent episode reward: [-20.709341476773794, 17.916667769909495, 17.916667769909495], time: 112.707
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 16.269004915369237, agent episode reward: [-22.47771456279463, 19.373359739081934, 19.373359739081934], time: 113.023
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 16.214667129654867, agent episode reward: [-21.13817129365936, 18.676419211657112, 18.676419211657112], time: 111.086
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 15.951814975368976, agent episode reward: [-21.965888078842852, 18.958851527105914, 18.958851527105914], time: 112.894
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 17.196723679220238, agent episode reward: [-22.332071797104987, 19.764397738162614, 19.764397738162614], time: 112.892
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 15.900161717267112, agent episode reward: [-22.86922611068281, 19.38469391397496, 19.38469391397496], time: 113.123
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 9.779257460455636, agent episode reward: [-21.61923529564811, 15.699246378051873, 15.699246378051873], time: 114.437
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 13.68714388372712, agent episode reward: [-23.203004336624844, 18.44507411017598, 18.44507411017598], time: 113.725
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 13.19077865968609, agent episode reward: [-29.935671683273966, 21.56322517148003, 21.56322517148003], time: 113.508
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -8.806470966746982, agent episode reward: [-16.254210752870673, 3.723869893061847, 3.723869893061847], time: 113.559
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -8.142391680592477, agent episode reward: [-14.67739224624675, 3.2675002828271373, 3.2675002828271373], time: 113.191
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -6.110921721373943, agent episode reward: [-17.627128987399935, 5.758103633012994, 5.758103633012994], time: 113.061
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -11.221182354126585, agent episode reward: [-22.43506984178151, 5.606943743827464, 5.606943743827464], time: 113.086
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 15.762272516202788, agent episode reward: [-24.5218821550983, 20.142077335650544, 20.142077335650544], time: 112.534
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 17.697395409664995, agent episode reward: [-22.21022096841232, 19.953808189038657, 19.953808189038657], time: 110.734
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 17.365798852347165, agent episode reward: [-24.196125420810976, 20.78096213657907, 20.78096213657907], time: 87.352
...Finished total of 60001 episodes.
