0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
3 bad agents
      adv rate for q_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
      adv rate for p_index :  3 [1e-05, 1e-05, 1e-05, 1e-05, 0.001, 0.001]
4 good agents
      adv rate for q_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  4 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
5 good agents
      adv rate for q_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
      adv rate for p_index :  5 [0.001, 0.001, 0.001, 0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 4 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.33895641254963, agent episode reward: [0.8477816715137328, 0.899393252217406, 0.8173825122337374, 0.8734779388370197, -14.05635151793119, -14.720640269420336], time: 198.152
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -17.872301939737977, agent episode reward: [1.9075723058316567, 1.4930907248288117, 1.8937015498834098, 1.9237117213879282, -12.150728432373242, -12.939649809296546], time: 258.653
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 9.735415563408504, agent episode reward: [3.381826516408201, 3.511011138216455, 3.3235601931113266, 3.440706695830281, -1.9878377950929518, -1.9338511850648092], time: 253.974
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 12.101798236462239, agent episode reward: [4.019110939276922, 4.161054760229077, 3.89347585625435, 4.201300838264319, -2.059263943519615, -2.113880214042818], time: 260.011
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 11.839261591575632, agent episode reward: [3.8672054164705476, 3.984391736211728, 3.98118839168675, 4.0231553149871635, -2.0651974212760615, -1.951481846504495], time: 248.698
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 15.535107794897609, agent episode reward: [5.07607225646232, 5.072531319213446, 5.05798455312262, 5.373619942971083, -2.4932010344649305, -2.5518992424069276], time: 258.831
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 21.54808749999778, agent episode reward: [6.970086994361163, 6.953946836191991, 7.005128267523414, 7.058614786378453, -3.2784696116563117, -3.16121977280093], time: 257.698
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 24.21320230746137, agent episode reward: [7.92729487718943, 7.927545984043951, 7.820522751146504, 7.782739998793553, -3.557123824898815, -3.6877774788132576], time: 260.49
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 28.127432244187613, agent episode reward: [9.216503614712142, 9.271839022878332, 9.204493205129705, 9.125354602123497, -4.217493069872871, -4.473265130783192], time: 261.67
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 31.056373519960047, agent episode reward: [10.059466618641448, 10.124332339456382, 10.131492289807039, 10.089437969602818, -3.8545665482106615, -5.493789149336974], time: 260.152
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 35.24802487201218, agent episode reward: [11.532663968663707, 11.381586165606521, 11.501413682325063, 11.487412249452095, -4.207863683140269, -6.447187510894939], time: 258.897
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 38.74374910938382, agent episode reward: [12.826773446533828, 12.534884038690812, 12.863145245253834, 12.762653327212824, -4.757458481833104, -7.486248466474375], time: 257.961
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 41.67851052028852, agent episode reward: [13.838439737442819, 13.674891295920887, 13.804729196194415, 13.790378938789457, -5.373162977594435, -8.056765670464626], time: 258.544
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 29.535177913282247, agent episode reward: [10.138930058682824, 9.953915350616837, 10.045168589303113, 10.066935690250782, -4.095827819127093, -6.573943956444217], time: 252.992
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 33.19261237314392, agent episode reward: [11.250103319083712, 11.097377904880263, 11.163094938881917, 11.175487917595223, -4.534844536167459, -6.958607171129735], time: 257.528
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 28.789775063503726, agent episode reward: [9.78233477286741, 9.603431146322826, 9.674038240650162, 9.653633420314017, -3.364649797816509, -6.55901271883418], time: 261.564
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 31.494075353540744, agent episode reward: [10.772462789218817, 10.562455481887001, 10.617626447059832, 10.592912720807192, -3.9619589776676047, -7.089423107764496], time: 259.718
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 30.2334866126919, agent episode reward: [10.00806553104126, 9.867512505553597, 9.843018029325446, 9.881465202850322, -3.1067168670302245, -6.259857789048502], time: 257.923
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 30.57557650384667, agent episode reward: [10.624987720741107, 10.450946565251865, 10.403015481832611, 10.415205982299955, -4.237398010648494, -7.081181235630376], time: 259.457
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 28.04708365676554, agent episode reward: [9.452765880347593, 9.337733526787867, 9.294594131609381, 9.34265600203527, -3.6586945863804106, -5.721971297634164], time: 261.329
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 20.609166326972655, agent episode reward: [7.329305424166143, 7.235096049631589, 7.154649424450085, 7.135633319463053, -4.239479415012674, -4.00603847572554], time: 261.669
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 14.79925581830232, agent episode reward: [5.7638935916065055, 5.650748368823912, 5.562455860464799, 5.5298054928335985, -3.286665688093537, -4.420981807332955], time: 257.723
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 15.612746152207865, agent episode reward: [6.101802960900591, 5.985488760409429, 5.905239523732335, 5.964171062902535, -4.111497538192287, -4.232458617544739], time: 258.214
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 14.67339692783418, agent episode reward: [5.703610314616541, 5.652061577212478, 5.591255820734777, 5.602341800330349, -3.575961637522711, -4.299910947537254], time: 261.761
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 13.756605021588125, agent episode reward: [5.938467845921024, 5.876880382423054, 5.805134191003929, 5.729194270930553, -4.365389395531968, -5.227682273158467], time: 260.533
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 18.7403651426059, agent episode reward: [6.954299907456721, 6.76337439948829, 6.6918001728654755, 6.713308898344005, -3.0249802685955225, -5.357437966953071], time: 260.731
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 16.89582904297208, agent episode reward: [6.6119327725777, 6.493821443400175, 6.420782633854984, 6.425899024595614, -4.219158887337089, -4.837447944119304], time: 259.958
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 18.17390328894481, agent episode reward: [6.688106229253777, 6.537450641980215, 6.405274133172672, 6.428878352301098, -3.718353019803464, -4.16745304795949], time: 258.878
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 20.155133916868117, agent episode reward: [7.21306013964029, 6.999039155356611, 6.968501345522199, 6.965591735706343, -3.3727550504815875, -4.618303408875743], time: 261.657
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 22.562567157637023, agent episode reward: [7.8562334915674565, 7.696996793457643, 7.706309283207648, 7.686252063500522, -3.335623363618027, -5.04760111047822], time: 260.317
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 21.14892262802907, agent episode reward: [7.291585506607673, 7.093433349345292, 7.096261563689655, 7.084993620087753, -2.6289567535094833, -4.788394658191821], time: 256.878
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 21.490358411201612, agent episode reward: [7.476488083874457, 7.346749341726925, 7.344514949712701, 7.291885295128062, -3.278386425637154, -4.690892833603378], time: 254.663
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 25.781331200327074, agent episode reward: [8.727652268040957, 8.600530476138028, 8.616565107348691, 8.585784831802059, -3.7679537480741985, -4.981247734928464], time: 252.704
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 22.666625646885986, agent episode reward: [7.783913042688428, 7.729284547586589, 7.675361770465074, 7.6725912812266115, -3.072996653254885, -5.1215283418258295], time: 251.977
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 22.772413634047844, agent episode reward: [7.897291268995774, 7.787875801308149, 7.795871391837452, 7.804352672752095, -3.152555062648891, -5.360422438196735], time: 255.105
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 23.82340032796144, agent episode reward: [8.051459711847098, 7.9993894912197065, 7.944584060374305, 7.888816156586843, -2.9962436258984133, -5.064605466168101], time: 256.493
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 23.436910905355177, agent episode reward: [8.028983992067435, 7.984455485910551, 7.887813193248624, 7.967635565288407, -3.2784408991746146, -5.153536431985225], time: 257.583
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 23.6889744512983, agent episode reward: [8.171570311152802, 8.068162787447937, 8.033886719944995, 8.05629359133832, -3.14339507421076, -5.497543884374994], time: 254.621
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 22.853272012093583, agent episode reward: [8.01159657134453, 7.818716929979158, 7.796378467463641, 7.905309225937422, -2.919832704041656, -5.758896478589508], time: 261.431
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 24.2477523352883, agent episode reward: [8.454933088615123, 8.324418735909077, 8.224644895976635, 8.338842783052703, -2.8416290523576873, -6.253458115907553], time: 262.322
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 23.622715551442997, agent episode reward: [8.12854625363695, 7.976270517046332, 7.920404832396208, 7.968523449935441, -2.5963673675091807, -5.7746621340627495], time: 259.383
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 21.056937555615317, agent episode reward: [7.753440219840213, 7.6834759358569285, 7.495580604700367, 7.63083517061704, -3.739321700438797, -5.767072674960431], time: 259.497
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 21.95207392164952, agent episode reward: [7.988905516016767, 7.887026569168015, 7.631145804373708, 7.816120058769748, -2.989644780002288, -6.38147924667643], time: 253.873
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 22.061962754766377, agent episode reward: [8.62678207989307, 8.449327950559605, 8.313581026067212, 8.376447079652714, -3.7636614422964563, -7.940513939109763], time: 256.949
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 22.310690664815915, agent episode reward: [8.480147514952124, 8.29858200083754, 8.11285047723915, 8.183532197332399, -3.3239874986494033, -7.440434026895893], time: 263.158
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 24.657973047508705, agent episode reward: [9.243837251811485, 9.017292374119222, 8.745442624838581, 8.89051094807222, -3.460768804244807, -7.7783413470879985], time: 263.53
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 27.204722382417, agent episode reward: [10.361300980833892, 10.124822416666339, 9.977040668592315, 9.924384473074953, -3.2554510227510023, -9.927375133999496], time: 262.743
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 26.93945311231579, agent episode reward: [9.932566510803392, 9.644749412585618, 9.434704232575331, 9.55021729407851, -3.3420270119752375, -8.280757325751825], time: 259.237
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 26.30410978144045, agent episode reward: [9.70699470881556, 9.485654244274912, 9.050316207666372, 9.286688398886012, -3.5836685421287813, -7.641875236073627], time: 259.778
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 24.363545473800396, agent episode reward: [9.575560221526331, 9.286666808518735, 8.979478516219146, 9.124916735624764, -4.255478085228939, -8.347598722859638], time: 261.966
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 29.795116792463492, agent episode reward: [11.247750689573557, 10.876970091819235, 10.576744468978115, 10.706145297945248, -4.074746467253986, -9.537747288598677], time: 260.408
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 35.024544931986675, agent episode reward: [12.563495040048846, 12.186161584602731, 11.873094158446278, 11.916776471408848, -2.788576448098783, -10.726405874421244], time: 253.554
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 33.60379044960752, agent episode reward: [12.084666944500936, 11.603561853661198, 11.25585250514103, 11.382074964488073, -2.592860892740456, -10.129504925443264], time: 257.233
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 32.83338055486783, agent episode reward: [11.680505782898665, 11.152823506746412, 10.889917183065759, 10.997371517249334, -2.2984924332117926, -9.588745001880545], time: 261.547
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 32.72360342031581, agent episode reward: [11.57457956793914, 11.129709617696424, 10.954529852055428, 10.999409609444738, -2.585296557395953, -9.349328669423961], time: 261.633
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 31.997278864676517, agent episode reward: [11.409829401216149, 10.924800352082464, 10.685132525730118, 10.651578028434363, -2.747069639701681, -8.926991803084897], time: 260.001
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 32.823746154173186, agent episode reward: [11.734960776177287, 11.296225914334629, 10.922150173819707, 11.048392308968651, -2.8686732941581545, -9.309309724968939], time: 259.574
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 31.381201496877228, agent episode reward: [11.010453377761106, 10.685807365617308, 10.383126754334995, 10.363194634970395, -2.2903044512628616, -8.771076184543714], time: 254.189
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 40.663576134637815, agent episode reward: [14.186266097153734, 13.726413608026727, 13.679961189420101, 13.446203623979063, -2.696450766936801, -11.67881761700501], time: 250.978
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 39.71476221774747, agent episode reward: [13.753080426707431, 13.338291012350478, 13.232439022534203, 13.045770857715686, -2.5036230487481097, -11.151196052812212], time: 232.088
...Finished total of 60001 episodes.
