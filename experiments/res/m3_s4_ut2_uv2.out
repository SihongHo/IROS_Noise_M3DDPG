0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -22.838762213751867, agent episode reward: [-38.839667930340994, 8.000452858294562, 8.000452858294562], time: 53.54
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -24.44768147043299, agent episode reward: [-28.27909970159395, 1.915709115580482, 1.915709115580482], time: 83.76
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -2.100364572145664, agent episode reward: [-12.181985307250422, 5.040810367552378, 5.040810367552378], time: 82.485
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 1.791971085402183, agent episode reward: [-10.709980567809096, 6.250975826605639, 6.250975826605639], time: 83.092
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 3.344486600339995, agent episode reward: [-10.231794162931592, 6.7881403816357935, 6.7881403816357935], time: 84.445
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 3.9889326417427657, agent episode reward: [-9.829274688796792, 6.909103665269779, 6.909103665269779], time: 83.05
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 3.415319280241657, agent episode reward: [-9.6030823455654, 6.509200812903529, 6.509200812903529], time: 83.77
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 2.7506262466475255, agent episode reward: [-10.11193038925976, 6.431278317953643, 6.431278317953643], time: 83.208
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 2.734287975079072, agent episode reward: [-10.280572550563983, 6.507430262821528, 6.507430262821528], time: 83.888
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 1.7345170699867596, agent episode reward: [-11.502636511072613, 6.618576790529686, 6.618576790529686], time: 84.22
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 1.548918106746811, agent episode reward: [-11.74117193642215, 6.6450450215844805, 6.6450450215844805], time: 85.778
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 1.047725624407786, agent episode reward: [-11.516064125552345, 6.2818948749800665, 6.2818948749800665], time: 84.376
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 0.6632516384701725, agent episode reward: [-11.969229628519336, 6.316240633494755, 6.316240633494755], time: 83.145
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 0.03800554809414831, agent episode reward: [-12.312197110319689, 6.175101329206918, 6.175101329206918], time: 82.907
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 0.8379144202599652, agent episode reward: [-12.104887756540135, 6.47140108840005, 6.47140108840005], time: 83.92
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 0.7778576980998461, agent episode reward: [-12.220765806824456, 6.49931175246215, 6.49931175246215], time: 84.242
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 0.3228589678166618, agent episode reward: [-11.957242460056026, 6.140050713936345, 6.140050713936345], time: 83.415
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 0.2761998214257913, agent episode reward: [-11.55083469606208, 5.913517258743935, 5.913517258743935], time: 83.425
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 0.18990360368639825, agent episode reward: [-11.269240315420674, 5.729571959553537, 5.729571959553537], time: 84.097
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 0.2814250135544937, agent episode reward: [-12.92792008542284, 6.604672549488667, 6.604672549488667], time: 84.111
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 0.054664490599713275, agent episode reward: [-11.999270010257513, 6.026967250428613, 6.026967250428613], time: 85.042
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -0.3167459261089841, agent episode reward: [-11.487422777801791, 5.585338425846404, 5.585338425846404], time: 83.161
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -0.6268164937393232, agent episode reward: [-11.876331683280307, 5.624757594770492, 5.624757594770492], time: 82.792
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -0.8084146466824991, agent episode reward: [-12.130673671959817, 5.661129512638659, 5.661129512638659], time: 84.299
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -1.073933882014957, agent episode reward: [-11.697572426511316, 5.31181927224818, 5.31181927224818], time: 83.276
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -1.4829487223100546, agent episode reward: [-11.996581502160954, 5.256816389925451, 5.256816389925451], time: 85.251
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -0.7297248807853931, agent episode reward: [-12.145832450841423, 5.708053785028015, 5.708053785028015], time: 84.055
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -0.5653701392793249, agent episode reward: [-11.418123843659282, 5.42637685218998, 5.42637685218998], time: 83.813
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -0.7683620594340257, agent episode reward: [-11.784426177089639, 5.5080320588278076, 5.5080320588278076], time: 83.674
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -0.606856658618035, agent episode reward: [-12.304810485212762, 5.848976913297363, 5.848976913297363], time: 84.261
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -0.15482315172944355, agent episode reward: [-12.112016154485547, 5.9785965013780515, 5.9785965013780515], time: 83.966
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -0.6932836582741024, agent episode reward: [-12.302777449828103, 5.804746895777, 5.804746895777], time: 85.559
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -0.4765243825182979, agent episode reward: [-12.930709052681852, 6.227092335081776, 6.227092335081776], time: 84.952
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -1.238120198084308, agent episode reward: [-11.856996832738941, 5.3094383173273165, 5.3094383173273165], time: 84.708
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -1.3568864209760572, agent episode reward: [-12.572239504722573, 5.607676541873257, 5.607676541873257], time: 88.001
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -2.231781437962978, agent episode reward: [-12.180076819866681, 4.974147690951852, 4.974147690951852], time: 87.226
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -1.6961548148068841, agent episode reward: [-13.501096828593203, 5.902471006893158, 5.902471006893158], time: 85.991
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -3.8456844453028083, agent episode reward: [-16.468949010508002, 6.311632282602597, 6.311632282602597], time: 86.846
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -3.015053321816761, agent episode reward: [-12.485965409971971, 4.735456044077606, 4.735456044077606], time: 86.087
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -2.1570194872977435, agent episode reward: [-12.182609768523015, 5.012795140612636, 5.012795140612636], time: 81.508
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -2.1751920338672615, agent episode reward: [-13.408202830922287, 5.6165053985275115, 5.6165053985275115], time: 77.146
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -1.63868202629997, agent episode reward: [-14.012748808436681, 6.187033391068356, 6.187033391068356], time: 77.123
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -1.4419224407214446, agent episode reward: [-13.647681243640545, 6.102879401459551, 6.102879401459551], time: 80.201
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -0.24817383548362068, agent episode reward: [-13.277128187671703, 6.514477176094042, 6.514477176094042], time: 81.18
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -1.1534418290436874, agent episode reward: [-13.432596110998627, 6.13957714097747, 6.13957714097747], time: 81.333
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -1.342360979845157, agent episode reward: [-12.612134046636056, 5.634886533395449, 5.634886533395449], time: 79.794
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -1.665210174659831, agent episode reward: [-13.488977419073603, 5.911883622206886, 5.911883622206886], time: 80.107
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -2.0263090226329292, agent episode reward: [-13.722160465676087, 5.8479257215215785, 5.8479257215215785], time: 82.421
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -1.463310579958525, agent episode reward: [-13.121840362683045, 5.829264891362262, 5.829264891362262], time: 77.51
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -1.4506661118892066, agent episode reward: [-12.946879057051444, 5.748106472581117, 5.748106472581117], time: 77.409
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -1.539743906820178, agent episode reward: [-13.572092439681459, 6.0161742664306415, 6.0161742664306415], time: 77.071
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -1.3855501176165583, agent episode reward: [-14.018944259374937, 6.31669707087919, 6.31669707087919], time: 77.27
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -1.9533658229682351, agent episode reward: [-12.99175522409243, 5.519194700562097, 5.519194700562097], time: 77.55
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -1.764402681779659, agent episode reward: [-13.093387509780333, 5.664492414000337, 5.664492414000337], time: 79.858
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -1.9110162350421471, agent episode reward: [-13.943764066270969, 6.016373915614411, 6.016373915614411], time: 77.657
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -0.7894792962786296, agent episode reward: [-13.595032506286339, 6.402776605003854, 6.402776605003854], time: 77.539
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -0.8133548855982535, agent episode reward: [-14.377800435045584, 6.782222774723665, 6.782222774723665], time: 77.546
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -0.7009544934979498, agent episode reward: [-14.20399681947005, 6.75152116298605, 6.75152116298605], time: 74.795
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -1.0221360185533122, agent episode reward: [-14.291544455135687, 6.634704218291187, 6.634704218291187], time: 75.659
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -0.8954447264116566, agent episode reward: [-14.559458526879938, 6.832006900234141, 6.832006900234141], time: 74.436
...Finished total of 60001 episodes.
