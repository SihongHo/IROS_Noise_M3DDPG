0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -24.52192420354503, agent episode reward: [-24.27630822198845, -0.12280799077829109, -0.12280799077829109], time: 56.829
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.03353129817043, agent episode reward: [-16.467701645853165, -1.7829148261586316, -1.7829148261586316], time: 105.803
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -2.2646438109349103, agent episode reward: [-1.2265805768999387, -0.5190316170174856, -0.5190316170174856], time: 111.773
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -3.7417431973024913, agent episode reward: [-2.318887678427101, -0.7114277594376952, -0.7114277594376952], time: 111.175
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -2.347787867528223, agent episode reward: [-4.516117155223548, 1.0841646438476629, 1.0841646438476629], time: 111.094
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -4.460820621694011, agent episode reward: [-3.949452460132261, -0.25568408078087446, -0.25568408078087446], time: 111.789
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -9.341176572544803, agent episode reward: [-9.592076522240548, 0.12544997484787138, 0.12544997484787138], time: 112.59
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -12.985888327978063, agent episode reward: [-16.353843270641132, 1.6839774713315334, 1.6839774713315334], time: 112.338
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -9.618729728401734, agent episode reward: [-15.911489864188788, 3.146380067893528, 3.146380067893528], time: 112.387
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -10.18177544319481, agent episode reward: [-14.024707216817466, 1.9214658868113288, 1.9214658868113288], time: 112.057
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -11.197129270844831, agent episode reward: [-13.900589440417177, 1.3517300847861715, 1.3517300847861715], time: 111.726
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -9.06454545370326, agent episode reward: [-16.394877037541395, 3.6651657919190685, 3.6651657919190685], time: 112.387
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -11.917195898936344, agent episode reward: [-25.3588985878117, 6.72085134443768, 6.72085134443768], time: 111.425
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -0.2251978979373399, agent episode reward: [-26.945515187512065, 13.360158644787363, 13.360158644787363], time: 112.134
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -7.017416587029844, agent episode reward: [-17.021679644315054, 5.002131528642604, 5.002131528642604], time: 112.625
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 3.1658099531215327, agent episode reward: [-14.665507054882006, 8.915658504001769, 8.915658504001769], time: 111.908
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 4.779811032939551, agent episode reward: [-10.977813902937255, 7.8788124679384035, 7.8788124679384035], time: 112.382
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 4.180724036278668, agent episode reward: [-10.687028491228627, 7.433876263753647, 7.433876263753647], time: 112.985
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 5.544444934525189, agent episode reward: [-11.499549049482196, 8.521996992003693, 8.521996992003693], time: 113.067
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 3.77835413775105, agent episode reward: [-12.8395027294966, 8.308928433623826, 8.308928433623826], time: 111.702
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -1.0206022690951113, agent episode reward: [-11.137155772004961, 5.058276751454923, 5.058276751454923], time: 111.429
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 4.639258675275246, agent episode reward: [-11.842029224585287, 8.240643949930266, 8.240643949930266], time: 111.7
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 6.166018865434177, agent episode reward: [-12.79639097860294, 9.481204922018557, 9.481204922018557], time: 112.498
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 8.40319457436562, agent episode reward: [-13.31869427911987, 10.860944426742746, 10.860944426742746], time: 112.837
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 8.550547891128767, agent episode reward: [-13.405522664386309, 10.97803527775754, 10.97803527775754], time: 111.501
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 6.233153131839453, agent episode reward: [-11.871321009662855, 9.052237070751154, 9.052237070751154], time: 111.562
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 8.812648692026045, agent episode reward: [-18.732366503780295, 13.772507597903168, 13.772507597903168], time: 111.873
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 1.018447307077831, agent episode reward: [-23.768647915699578, 12.393547611388703, 12.393547611388703], time: 112.598
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 16.880705203434488, agent episode reward: [-22.153791939566965, 19.517248571500726, 19.517248571500726], time: 111.748
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 18.389083507463607, agent episode reward: [-22.486004509815594, 20.4375440086396, 20.4375440086396], time: 112.722
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 18.76761719402982, agent episode reward: [-23.01287276638278, 20.890244980206305, 20.890244980206305], time: 110.912
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 17.181007776055388, agent episode reward: [-21.94859688161633, 19.564802328835857, 19.564802328835857], time: 110.88
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 18.385534271948767, agent episode reward: [-22.547780801910353, 20.46665753692956, 20.46665753692956], time: 112.662
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 16.774239459346322, agent episode reward: [-21.071598708033473, 18.922919083689898, 18.922919083689898], time: 113.032
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 16.669681003424106, agent episode reward: [-20.799697449675755, 18.73468922654993, 18.73468922654993], time: 111.34
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 15.077195054342502, agent episode reward: [-20.020996446313685, 17.549095750328092, 17.549095750328092], time: 112.142
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 16.57230411461045, agent episode reward: [-20.913918880989048, 18.74311149779975, 18.74311149779975], time: 111.892
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 13.566899967476479, agent episode reward: [-20.581333949887917, 17.0741169586822, 17.0741169586822], time: 113.638
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 15.395921754764062, agent episode reward: [-19.825729792785626, 17.61082577377484, 17.61082577377484], time: 111.062
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 17.344550084974486, agent episode reward: [-21.099900455462397, 19.222225270218438, 19.222225270218438], time: 112.591
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 15.544008975413439, agent episode reward: [-19.81647729254313, 17.680243133978284, 17.680243133978284], time: 112.854
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 15.708333410493857, agent episode reward: [-20.514989194675188, 18.111661302584523, 18.111661302584523], time: 113.04
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 6.90704600164656, agent episode reward: [-17.256639866861782, 12.081842934254174, 12.081842934254174], time: 112.189
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 6.917981298638336, agent episode reward: [-15.197181283344404, 11.057581290991369, 11.057581290991369], time: 112.615
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 7.646445205447005, agent episode reward: [-14.028752523653296, 10.837598864550152, 10.837598864550152], time: 112.869
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 5.306247802816258, agent episode reward: [-12.912989058513736, 9.109618430664996, 9.109618430664996], time: 113.031
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 7.674958192977319, agent episode reward: [-12.514788256709176, 10.09487322484325, 10.09487322484325], time: 112.787
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 5.973209998989351, agent episode reward: [-12.471995110358465, 9.222602554673907, 9.222602554673907], time: 113.601
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 8.14357880044768, agent episode reward: [-13.1139152636947, 10.62874703207119, 10.62874703207119], time: 112.405
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 6.638635464643619, agent episode reward: [-13.485886252976314, 10.06226085880997, 10.06226085880997], time: 113.347
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 5.133780725962664, agent episode reward: [-13.255410962440745, 9.194595844201704, 9.194595844201704], time: 111.286
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 7.8586282767975035, agent episode reward: [-13.559340980582613, 10.708984628690057, 10.708984628690057], time: 113.311
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 7.534459668993936, agent episode reward: [-14.774902868690438, 11.154681268842186, 11.154681268842186], time: 112.481
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 4.778241564161758, agent episode reward: [-13.974193012403859, 9.376217288282808, 9.376217288282808], time: 111.205
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 5.596102776068633, agent episode reward: [-13.498241180350993, 9.547171978209812, 9.547171978209812], time: 112.363
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 3.388372916794981, agent episode reward: [-13.873139948344281, 8.630756432569632, 8.630756432569632], time: 113.278
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 6.064448281951639, agent episode reward: [-13.924412164248883, 9.99443022310026, 9.99443022310026], time: 112.715
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 4.334555465834234, agent episode reward: [-14.374544643284601, 9.354550054559416, 9.354550054559416], time: 112.536
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 8.106077620808742, agent episode reward: [-15.52188216410617, 11.813979892457457, 11.813979892457457], time: 113.878
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 8.230325295165427, agent episode reward: [-14.997249563906669, 11.61378742953605, 11.61378742953605], time: 112.847
...Finished total of 60001 episodes.
