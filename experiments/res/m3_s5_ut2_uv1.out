0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -0.3703947100230327, agent episode reward: [2.49, 2.49, 2.49, -7.840394710023033], time: 91.837
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -2.604043756088802, agent episode reward: [3.43, 3.43, 3.43, -12.894043756088802], time: 124.215
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 2.786653647292032, agent episode reward: [4.16, 4.16, 4.16, -9.693346352707968], time: 124.383
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 7.3990972214075645, agent episode reward: [4.34, 4.34, 4.34, -5.620902778592435], time: 124.028
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 9.472997360306938, agent episode reward: [5.09, 5.09, 5.09, -5.797002639693064], time: 125.09
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 15.51563606238267, agent episode reward: [8.14, 8.14, 8.14, -8.904363937617331], time: 123.353
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 23.266364457584785, agent episode reward: [11.85, 11.85, 11.85, -12.283635542415215], time: 124.428
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 34.1711399058662, agent episode reward: [17.21, 17.21, 17.21, -17.4588600941338], time: 123.873
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 37.62736105070968, agent episode reward: [19.03, 19.03, 19.03, -19.462638949290316], time: 127.511
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 33.25304212768622, agent episode reward: [17.09, 17.09, 17.09, -18.01695787231378], time: 124.919
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 24.704885737969803, agent episode reward: [13.3, 13.3, 13.3, -15.195114262030199], time: 125.435
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 12.936397384151737, agent episode reward: [8.65, 8.65, 8.65, -13.013602615848262], time: 124.373
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 8.258385847396184, agent episode reward: [7.1, 7.1, 7.1, -13.041614152603819], time: 124.238
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 9.133228221534573, agent episode reward: [7.35, 7.35, 7.35, -12.916771778465424], time: 124.632
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 12.402842693164512, agent episode reward: [8.79, 8.79, 8.79, -13.967157306835489], time: 125.89
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 9.816487181821481, agent episode reward: [7.47, 7.47, 7.47, -12.59351281817852], time: 124.227
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 16.20152917543364, agent episode reward: [9.5, 9.5, 9.5, -12.298470824566357], time: 123.917
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 13.518978801481891, agent episode reward: [8.23, 8.23, 8.23, -11.171021198518108], time: 124.98
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 15.114931194258014, agent episode reward: [8.67, 8.67, 8.67, -10.895068805741987], time: 124.349
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 14.878029153146748, agent episode reward: [8.43, 8.43, 8.43, -10.411970846853254], time: 123.819
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 14.864447557225434, agent episode reward: [8.47, 8.47, 8.47, -10.545552442774566], time: 124.572
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 15.019769436573021, agent episode reward: [8.59, 8.59, 8.59, -10.75023056342698], time: 124.859
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 14.677924834229403, agent episode reward: [8.35, 8.35, 8.35, -10.372075165770594], time: 129.814
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 19.473268792025614, agent episode reward: [10.56, 10.56, 10.56, -12.206731207974386], time: 129.807
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 20.441067303408946, agent episode reward: [11.04, 11.04, 11.04, -12.678932696591056], time: 129.893
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 24.01091183018411, agent episode reward: [13.13, 13.13, 13.13, -15.379088169815889], time: 126.561
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 25.310558253732243, agent episode reward: [13.85, 13.85, 13.85, -16.239441746267758], time: 122.605
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 26.50486585784138, agent episode reward: [14.7, 14.7, 14.7, -17.595134142158617], time: 117.834
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 31.270189778232155, agent episode reward: [17.26, 17.26, 17.26, -20.509810221767847], time: 116.935
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 31.521131518414425, agent episode reward: [18.06, 18.06, 18.06, -22.658868481585575], time: 117.183
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 37.12049715588778, agent episode reward: [21.42, 21.42, 21.42, -27.13950284411221], time: 120.89
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 30.575801309527986, agent episode reward: [18.14, 18.14, 18.14, -23.844198690472016], time: 117.171
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 26.62766619998599, agent episode reward: [16.08, 16.08, 16.08, -21.61233380001401], time: 118.92
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 30.876334526115908, agent episode reward: [17.54, 17.54, 17.54, -21.743665473884093], time: 116.671
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 27.747124831317706, agent episode reward: [16.03, 16.03, 16.03, -20.342875168682294], time: 116.275
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 35.98441380052767, agent episode reward: [20.11, 20.11, 20.11, -24.34558619947233], time: 116.598
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 27.11922490310808, agent episode reward: [15.99, 15.99, 15.99, -20.850775096891923], time: 116.51
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 23.143927558434655, agent episode reward: [14.64, 14.64, 14.64, -20.776072441565347], time: 118.185
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 24.9945817869974, agent episode reward: [14.87, 14.87, 14.87, -19.6154182130026], time: 113.708
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 21.6024414069408, agent episode reward: [13.73, 13.73, 13.73, -19.5875585930592], time: 112.636
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 24.756604339977546, agent episode reward: [15.54, 15.54, 15.54, -21.863395660022455], time: 90.838
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 22.102648686591362, agent episode reward: [13.82, 13.82, 13.82, -19.35735131340864], time: 85.903
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 24.96227705573909, agent episode reward: [15.3, 15.3, 15.3, -20.93772294426091], time: 84.084
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 22.50632945106315, agent episode reward: [13.49, 13.49, 13.49, -17.96367054893685], time: 85.864
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 20.997385173322925, agent episode reward: [12.77, 12.77, 12.77, -17.312614826677077], time: 89.725
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 22.933143452509356, agent episode reward: [13.55, 13.55, 13.55, -17.716856547490643], time: 88.773
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 22.54540148879088, agent episode reward: [13.62, 13.62, 13.62, -18.314598511209116], time: 88.644
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 22.57742491757765, agent episode reward: [13.56, 13.56, 13.56, -18.10257508242235], time: 87.837
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 25.938193333085263, agent episode reward: [15.0, 15.0, 15.0, -19.061806666914737], time: 83.761
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 23.02527153796074, agent episode reward: [13.64, 13.64, 13.64, -17.89472846203926], time: 83.748
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 26.088163260174202, agent episode reward: [15.33, 15.33, 15.33, -19.9018367398258], time: 83.165
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 22.51068588901606, agent episode reward: [13.39, 13.39, 13.39, -17.659314110983942], time: 88.551
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 28.52818106805012, agent episode reward: [16.28, 16.28, 16.28, -20.311818931949883], time: 84.37
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 27.715350983223477, agent episode reward: [15.91, 15.91, 15.91, -20.014649016776524], time: 87.091
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 23.796702854994066, agent episode reward: [13.91, 13.91, 13.91, -17.933297145005934], time: 86.518
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 24.39806725638471, agent episode reward: [14.2, 14.2, 14.2, -18.201932743615288], time: 90.374
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 21.99693537253624, agent episode reward: [12.78, 12.78, 12.78, -16.34306462746376], time: 86.286
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 23.50890094608262, agent episode reward: [13.78, 13.78, 13.78, -17.83109905391738], time: 84.552
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 23.551163036783276, agent episode reward: [13.73, 13.73, 13.73, -17.638836963216722], time: 89.263
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 22.36523956313862, agent episode reward: [13.51, 13.51, 13.51, -18.16476043686138], time: 87.212
...Finished total of 60001 episodes.
