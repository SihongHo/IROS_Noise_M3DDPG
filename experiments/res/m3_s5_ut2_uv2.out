0 bad agents
      adv rate for q_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 1e-05, 1e-05, 0.001]
1 bad agents
      adv rate for q_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  1 [1e-05, 1e-05, 1e-05, 0.001]
2 bad agents
      adv rate for q_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
      adv rate for p_index :  2 [1e-05, 1e-05, 1e-05, 0.001]
3 good agents
      adv rate for q_index :  3 [0.001, 0.001, 0.001, 1e-05]
      adv rate for p_index :  3 [0.001, 0.001, 0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 3 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -5.617655640909663, agent episode reward: [2.17, 2.17, 2.17, -12.127655640909662], time: 93.064
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -7.5509513866753695, agent episode reward: [4.3, 4.3, 4.3, -20.45095138667537], time: 124.919
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: 4.035771249000595, agent episode reward: [4.46, 4.46, 4.46, -9.344228750999406], time: 124.573
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: 10.416748376192638, agent episode reward: [5.86, 5.86, 5.86, -7.163251623807362], time: 124.634
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: 11.776302990863547, agent episode reward: [6.05, 6.05, 6.05, -6.3736970091364515], time: 124.655
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: 12.632360176718851, agent episode reward: [6.54, 6.54, 6.54, -6.987639823281147], time: 125.39
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: 14.815525119111811, agent episode reward: [7.63, 7.63, 7.63, -8.074474880888191], time: 127.634
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: 19.96566099412697, agent episode reward: [10.25, 10.25, 10.25, -10.78433900587303], time: 125.442
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: 27.418518080946427, agent episode reward: [13.82, 13.82, 13.82, -14.041481919053574], time: 126.32
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: 41.16812722477087, agent episode reward: [20.94, 20.94, 20.94, -21.651872775229126], time: 126.179
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: 48.15661541716701, agent episode reward: [24.38, 24.38, 24.38, -24.983384582832986], time: 124.88
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: 43.141838469493685, agent episode reward: [22.57, 22.57, 22.57, -24.568161530506313], time: 125.013
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 31.070993043278243, agent episode reward: [16.77, 16.77, 16.77, -19.23900695672176], time: 125.323
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 17.693180443206053, agent episode reward: [11.19, 11.19, 11.19, -15.87681955679395], time: 124.941
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 8.787388550993066, agent episode reward: [7.74, 7.74, 7.74, -14.432611449006934], time: 125.425
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 8.649315390847546, agent episode reward: [7.66, 7.66, 7.66, -14.330684609152454], time: 125.826
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 2.9846955188451694, agent episode reward: [5.32, 5.32, 5.32, -12.975304481154831], time: 125.852
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 4.2897360392789725, agent episode reward: [5.91, 5.91, 5.91, -13.44026396072103], time: 124.895
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 4.444982600768156, agent episode reward: [5.62, 5.62, 5.62, -12.415017399231845], time: 126.24
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 3.001463094489351, agent episode reward: [4.7, 4.7, 4.7, -11.098536905510649], time: 125.546
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 5.670950158173953, agent episode reward: [5.71, 5.71, 5.71, -11.459049841826047], time: 128.251
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 6.59179586481217, agent episode reward: [5.84, 5.84, 5.84, -10.92820413518783], time: 125.415
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 6.494163685394129, agent episode reward: [5.67, 5.67, 5.67, -10.51583631460587], time: 130.266
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 3.8599523809298715, agent episode reward: [4.64, 4.64, 4.64, -10.060047619070126], time: 130.038
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 7.007362473141808, agent episode reward: [5.25, 5.25, 5.25, -8.742637526858193], time: 126.688
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 10.227632191946585, agent episode reward: [7.0, 7.0, 7.0, -10.772367808053415], time: 126.703
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 10.130006214499288, agent episode reward: [6.5, 6.5, 6.5, -9.369993785500712], time: 120.958
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 10.656938255053788, agent episode reward: [6.74, 6.74, 6.74, -9.563061744946209], time: 116.144
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 10.136191479039875, agent episode reward: [6.11, 6.11, 6.11, -8.193808520960127], time: 115.819
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 11.743348495533684, agent episode reward: [7.01, 7.01, 7.01, -9.286651504466315], time: 117.622
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 14.438083142409498, agent episode reward: [8.24, 8.24, 8.24, -10.281916857590502], time: 115.911
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 14.253839562221462, agent episode reward: [8.0, 8.0, 8.0, -9.746160437778538], time: 116.902
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 18.47955618925279, agent episode reward: [10.44, 10.44, 10.44, -12.84044381074721], time: 114.605
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 17.28161889586715, agent episode reward: [9.57, 9.57, 9.57, -11.428381104132848], time: 114.02
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 19.32956043499129, agent episode reward: [10.73, 10.73, 10.73, -12.86043956500871], time: 113.816
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 28.03270087113279, agent episode reward: [15.25, 15.25, 15.25, -17.71729912886721], time: 119.021
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 30.438411558301183, agent episode reward: [16.94, 16.94, 16.94, -20.38158844169882], time: 114.897
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 37.516778179051244, agent episode reward: [20.14, 20.14, 20.14, -22.903221820948758], time: 114.984
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 41.19019401437355, agent episode reward: [22.26, 22.26, 22.26, -25.589805985626437], time: 108.693
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 50.195600599147625, agent episode reward: [26.43, 26.43, 26.43, -29.094399400852375], time: 105.572
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 51.10983310325102, agent episode reward: [27.19, 27.19, 27.19, -30.460166896748984], time: 85.467
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 49.0792675191969, agent episode reward: [26.47, 26.47, 26.47, -30.330732480803103], time: 83.454
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 48.55475219758338, agent episode reward: [26.17, 26.17, 26.17, -29.955247802416615], time: 83.168
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 42.84141238406799, agent episode reward: [23.8, 23.8, 23.8, -28.558587615932], time: 83.563
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 41.29309630846249, agent episode reward: [22.83, 22.83, 22.83, -27.19690369153751], time: 86.004
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 31.138252548136414, agent episode reward: [18.82, 18.82, 18.82, -25.321747451863587], time: 85.09
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 20.976584922309584, agent episode reward: [14.19, 14.19, 14.19, -21.59341507769042], time: 87.105
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 15.252712196579541, agent episode reward: [12.03, 12.03, 12.03, -20.83728780342046], time: 82.679
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 7.324477795563035, agent episode reward: [8.58, 8.58, 8.58, -18.415522204436964], time: 82.815
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 10.60876454182858, agent episode reward: [10.41, 10.41, 10.41, -20.62123545817142], time: 82.561
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 8.854039332767268, agent episode reward: [9.7, 9.7, 9.7, -20.245960667232733], time: 87.508
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: 11.218661662673902, agent episode reward: [10.06, 10.06, 10.06, -18.961338337326094], time: 84.064
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 13.983666593688195, agent episode reward: [11.34, 11.34, 11.34, -20.036333406311805], time: 83.382
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 18.886165820284813, agent episode reward: [12.88, 12.88, 12.88, -19.753834179715188], time: 82.656
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 18.553556928465028, agent episode reward: [12.4, 12.4, 12.4, -18.646443071534975], time: 85.601
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 23.638204394387692, agent episode reward: [14.69, 14.69, 14.69, -20.431795605612308], time: 85.384
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 25.52678321540838, agent episode reward: [15.36, 15.36, 15.36, -20.55321678459162], time: 87.866
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 23.37515431099939, agent episode reward: [14.18, 14.18, 14.18, -19.164845689000614], time: 83.583
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 20.99181616609177, agent episode reward: [12.78, 12.78, 12.78, -17.348183833908234], time: 83.804
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 18.672617530851078, agent episode reward: [11.4, 11.4, 11.4, -15.52738246914892], time: 83.915
...Finished total of 60001 episodes.
