0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -26.87773301413591, agent episode reward: [0.2602948418223209, -27.138027855958228], time: 41.041
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -20.54769874560587, agent episode reward: [-3.4335436990210337, -17.114155046584834], time: 54.102
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -13.803442509696662, agent episode reward: [-6.1684246558480815, -7.6350178538485824], time: 53.436
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -11.021894740322498, agent episode reward: [-3.871605288916453, -7.150289451406044], time: 53.401
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -10.964607249873973, agent episode reward: [-3.7903628357299284, -7.1742444141440425], time: 54.451
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -9.764209719803512, agent episode reward: [-2.721338802404747, -7.042870917398765], time: 55.228
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -9.945695702956124, agent episode reward: [-2.638677173969976, -7.307018528986146], time: 54.949
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -10.412291816257914, agent episode reward: [-2.7716935010388766, -7.640598315219037], time: 55.087
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -10.087796410523316, agent episode reward: [-2.635430357244042, -7.452366053279275], time: 54.555
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -10.260072175566542, agent episode reward: [-2.7087325787088137, -7.55133959685773], time: 55.13
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -9.718125720076468, agent episode reward: [-2.2776065625232547, -7.440519157553213], time: 54.395
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -9.80510330603259, agent episode reward: [-2.267278765686877, -7.537824540345713], time: 54.736
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: -9.625647822081206, agent episode reward: [-2.455994957013056, -7.169652865068149], time: 54.245
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: -9.63535787148902, agent episode reward: [-2.305434178706619, -7.329923692782401], time: 54.307
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: -9.76353888026134, agent episode reward: [-2.286868672458439, -7.476670207802901], time: 54.126
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: -9.883063170581945, agent episode reward: [-2.0339857380838486, -7.849077432498096], time: 54.31
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: -9.8249739525542, agent episode reward: [-2.6984202757154434, -7.126553676838757], time: 54.187
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: -9.719423825057588, agent episode reward: [-2.369441802585642, -7.349982022471944], time: 55.418
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: -9.505332760327924, agent episode reward: [-2.2577669595705236, -7.2475658007574015], time: 54.976
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: -9.850275445521245, agent episode reward: [-2.714487107913415, -7.13578833760783], time: 56.222
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: -9.858605461419481, agent episode reward: [-2.6535900289786802, -7.205015432440801], time: 55.098
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: -9.56917631855885, agent episode reward: [-2.2182357812771643, -7.350940537281686], time: 54.299
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: -9.755820934194224, agent episode reward: [-2.2831465540884035, -7.472674380105818], time: 54.761
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: -9.92351173611831, agent episode reward: [-2.130122279858108, -7.793389456260203], time: 54.325
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: -9.799711202281769, agent episode reward: [-2.0361638238478403, -7.763547378433929], time: 54.177
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: -9.523111462842436, agent episode reward: [-1.9049630273376865, -7.618148435504748], time: 54.058
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: -9.975778048514705, agent episode reward: [-2.2433367263384536, -7.732441322176251], time: 54.237
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: -9.992055927567801, agent episode reward: [-1.9548406109578917, -8.037215316609908], time: 54.061
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: -9.991692986839963, agent episode reward: [-2.192460433021961, -7.799232553818003], time: 54.703
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: -9.933224890966223, agent episode reward: [-2.057035169598937, -7.876189721367288], time: 54.134
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: -10.185922575635912, agent episode reward: [-2.3971387819333367, -7.788783793702576], time: 55.245
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: -10.10206543915939, agent episode reward: [-1.9834165957256806, -8.118648843433707], time: 54.931
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: -10.004689733541014, agent episode reward: [-2.2319657638469033, -7.77272396969411], time: 54.026
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: -9.995359175267742, agent episode reward: [-2.2057131923521585, -7.789645982915584], time: 54.921
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: -10.015275049736566, agent episode reward: [-1.7730650170122217, -8.242210032724344], time: 54.107
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: -9.985326714544765, agent episode reward: [-1.838338620785311, -8.146988093759454], time: 54.141
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: -10.223398003911381, agent episode reward: [-2.231225565388347, -7.992172438523035], time: 55.443
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: -10.340964771707801, agent episode reward: [-2.707563066410902, -7.633401705296897], time: 54.226
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: -10.306090704236295, agent episode reward: [-2.710671620762123, -7.595419083474174], time: 54.503
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: -10.20937943523302, agent episode reward: [-2.5666574920184813, -7.6427219432145375], time: 54.884
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: -10.052038635048458, agent episode reward: [-2.451319671320157, -7.600718963728303], time: 54.212
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: -10.318095807714332, agent episode reward: [-2.5240032853040906, -7.794092522410242], time: 54.258
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: -10.63320673143581, agent episode reward: [-2.821032948751037, -7.812173782684772], time: 54.46
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: -10.278705829937701, agent episode reward: [-2.262996591584206, -8.015709238353494], time: 54.533
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: -10.487263899842299, agent episode reward: [-2.6077472641499577, -7.879516635692341], time: 54.367
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: -10.538992586417683, agent episode reward: [-2.568129941548558, -7.970862644869124], time: 55.805
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: -10.025101827816638, agent episode reward: [-2.1884763834434677, -7.83662544437317], time: 54.523
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: -10.090554485318641, agent episode reward: [-2.581115930934363, -7.509438554384278], time: 53.993
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: -10.45140502460127, agent episode reward: [-3.187271771909881, -7.264133252691391], time: 54.313
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: -10.45548861018548, agent episode reward: [-2.8658611514982733, -7.5896274586872075], time: 55.222
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: -10.538475198123335, agent episode reward: [-2.8629643708166204, -7.675510827306713], time: 55.993
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -10.549856254395364, agent episode reward: [-2.282771594067047, -8.267084660328315], time: 57.264
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: -10.618034786249959, agent episode reward: [-2.896960093945777, -7.721074692304183], time: 55.878
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: -10.74487180537184, agent episode reward: [-2.760567567858091, -7.984304237513748], time: 54.857
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: -10.766664642456265, agent episode reward: [-2.5163216458613324, -8.25034299659493], time: 57.559
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: -11.104789408202084, agent episode reward: [-2.8213586748216914, -8.283430733380394], time: 57.478
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: -11.257500380925219, agent episode reward: [-2.988722581660103, -8.268777799265116], time: 57.784
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: -11.522369711587963, agent episode reward: [-3.691205395332634, -7.831164316255327], time: 55.549
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: -11.088477702106674, agent episode reward: [-3.6859011170629947, -7.402576585043682], time: 55.727
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: -11.209932055555415, agent episode reward: [-3.662410651921904, -7.547521403633513], time: 57.296
...Finished total of 60001 episodes.
