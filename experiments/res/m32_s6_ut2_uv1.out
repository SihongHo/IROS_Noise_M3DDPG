0 bad agents
      adv rate for q_index :  0 [1e-05, 0.001, 0.001]
      adv rate for p_index :  0 [1e-05, 0.001, 0.001]
1 good agents
      adv rate for q_index :  1 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  1 [0.001, 1e-05, 1e-05]
2 good agents
      adv rate for q_index :  2 [0.001, 1e-05, 1e-05]
      adv rate for p_index :  2 [0.001, 1e-05, 1e-05]
Using good policy mmmaddpg and bad policy mmmaddpg with 1 adversaries
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
mmmaddpg vs mmmaddpg steps: 24975, episodes: 1000, mean episode reward: -25.341421534175502, agent episode reward: [-24.13911990213723, -0.601150816019135, -0.601150816019135], time: 72.329
mmmaddpg vs mmmaddpg steps: 49975, episodes: 2000, mean episode reward: -13.363003594279625, agent episode reward: [-9.73632201459724, -1.8133407898411935, -1.8133407898411935], time: 112.092
mmmaddpg vs mmmaddpg steps: 74975, episodes: 3000, mean episode reward: -7.301941694436775, agent episode reward: [-6.280102398145356, -0.5109196481457101, -0.5109196481457101], time: 112.086
mmmaddpg vs mmmaddpg steps: 99975, episodes: 4000, mean episode reward: -3.134727111780539, agent episode reward: [-3.6504330291966474, 0.2578529587080543, 0.2578529587080543], time: 112.477
mmmaddpg vs mmmaddpg steps: 124975, episodes: 5000, mean episode reward: -3.0521829457911025, agent episode reward: [-6.241407127228648, 1.5946120907187724, 1.5946120907187724], time: 111.498
mmmaddpg vs mmmaddpg steps: 149975, episodes: 6000, mean episode reward: -1.0493983625414112, agent episode reward: [-6.003005134514984, 2.476803385986787, 2.476803385986787], time: 111.325
mmmaddpg vs mmmaddpg steps: 174975, episodes: 7000, mean episode reward: -11.63622978183243, agent episode reward: [-15.947747543100384, 2.155758880633977, 2.155758880633977], time: 111.76
mmmaddpg vs mmmaddpg steps: 199975, episodes: 8000, mean episode reward: -9.945516622833669, agent episode reward: [-13.293722457704403, 1.6741029174353668, 1.6741029174353668], time: 111.36
mmmaddpg vs mmmaddpg steps: 224975, episodes: 9000, mean episode reward: -8.688846801273735, agent episode reward: [-13.461334549210422, 2.386243873968343, 2.386243873968343], time: 111.97
mmmaddpg vs mmmaddpg steps: 249975, episodes: 10000, mean episode reward: -14.487717295651375, agent episode reward: [-15.040914760859376, 0.27659873260400103, 0.27659873260400103], time: 111.704
mmmaddpg vs mmmaddpg steps: 274975, episodes: 11000, mean episode reward: -11.306591074997998, agent episode reward: [-13.995591219296806, 1.344500072149404, 1.344500072149404], time: 111.875
mmmaddpg vs mmmaddpg steps: 299975, episodes: 12000, mean episode reward: -9.046646721331221, agent episode reward: [-14.273286178226256, 2.613319728447517, 2.613319728447517], time: 113.087
mmmaddpg vs mmmaddpg steps: 324975, episodes: 13000, mean episode reward: 0.26844289630765683, agent episode reward: [-12.920391389377375, 6.5944171428425165, 6.5944171428425165], time: 112.232
mmmaddpg vs mmmaddpg steps: 349975, episodes: 14000, mean episode reward: 3.333075489860212, agent episode reward: [-12.13592421418594, 7.734499852023076, 7.734499852023076], time: 112.659
mmmaddpg vs mmmaddpg steps: 374975, episodes: 15000, mean episode reward: 4.543978849110968, agent episode reward: [-10.988327645128297, 7.766153247119635, 7.766153247119635], time: 113.362
mmmaddpg vs mmmaddpg steps: 399975, episodes: 16000, mean episode reward: 2.5944425841826333, agent episode reward: [-9.10519722012844, 5.849819902155537, 5.849819902155537], time: 113.156
mmmaddpg vs mmmaddpg steps: 424975, episodes: 17000, mean episode reward: 2.0578961703142915, agent episode reward: [-8.395439350853069, 5.2266677605836795, 5.2266677605836795], time: 112.906
mmmaddpg vs mmmaddpg steps: 449975, episodes: 18000, mean episode reward: 2.7280851336109535, agent episode reward: [-9.033881352626628, 5.8809832431187905, 5.8809832431187905], time: 112.915
mmmaddpg vs mmmaddpg steps: 474975, episodes: 19000, mean episode reward: 1.818119074066557, agent episode reward: [-9.81247148781439, 5.8152952809404725, 5.8152952809404725], time: 112.292
mmmaddpg vs mmmaddpg steps: 499975, episodes: 20000, mean episode reward: 0.7690998715432813, agent episode reward: [-9.981238826314463, 5.375169348928872, 5.375169348928872], time: 111.782
mmmaddpg vs mmmaddpg steps: 524975, episodes: 21000, mean episode reward: 2.35968073954729, agent episode reward: [-10.751532921899944, 6.555606830723617, 6.555606830723617], time: 112.851
mmmaddpg vs mmmaddpg steps: 549975, episodes: 22000, mean episode reward: 2.5405927963869943, agent episode reward: [-10.53405116291335, 6.537321979650173, 6.537321979650173], time: 113.304
mmmaddpg vs mmmaddpg steps: 574975, episodes: 23000, mean episode reward: 3.534475367542048, agent episode reward: [-12.815516193936064, 8.174995780739055, 8.174995780739055], time: 113.061
mmmaddpg vs mmmaddpg steps: 599975, episodes: 24000, mean episode reward: 1.6623778148588568, agent episode reward: [-14.490908820490858, 8.076643317674858, 8.076643317674858], time: 113.193
mmmaddpg vs mmmaddpg steps: 624975, episodes: 25000, mean episode reward: 5.164440404859964, agent episode reward: [-18.185196057268193, 11.674818231064076, 11.674818231064076], time: 112.291
mmmaddpg vs mmmaddpg steps: 649975, episodes: 26000, mean episode reward: 9.38722346893621, agent episode reward: [-22.574763778281298, 15.980993623608752, 15.980993623608752], time: 112.1
mmmaddpg vs mmmaddpg steps: 674975, episodes: 27000, mean episode reward: 19.64770270415495, agent episode reward: [-23.66659891143992, 21.657150807797432, 21.657150807797432], time: 112.027
mmmaddpg vs mmmaddpg steps: 699975, episodes: 28000, mean episode reward: 18.147237680854982, agent episode reward: [-22.536385058939697, 20.34181136989734, 20.34181136989734], time: 112.804
mmmaddpg vs mmmaddpg steps: 724975, episodes: 29000, mean episode reward: 18.486642746805366, agent episode reward: [-22.34576198549074, 20.416202366148056, 20.416202366148056], time: 111.955
mmmaddpg vs mmmaddpg steps: 749975, episodes: 30000, mean episode reward: 20.32729793046868, agent episode reward: [-23.959502222224664, 22.143400076346673, 22.143400076346673], time: 113.1
mmmaddpg vs mmmaddpg steps: 774975, episodes: 31000, mean episode reward: 18.792331508296883, agent episode reward: [-23.004672329606795, 20.898501918951837, 20.898501918951837], time: 111.152
mmmaddpg vs mmmaddpg steps: 799975, episodes: 32000, mean episode reward: 18.534144670723276, agent episode reward: [-23.255524995887033, 20.894834833305154, 20.894834833305154], time: 112.377
mmmaddpg vs mmmaddpg steps: 824975, episodes: 33000, mean episode reward: 17.891409500394666, agent episode reward: [-23.026354457419608, 20.458881978907137, 20.458881978907137], time: 112.855
mmmaddpg vs mmmaddpg steps: 849975, episodes: 34000, mean episode reward: 18.80177249598827, agent episode reward: [-23.572015556534712, 21.18689402626149, 21.18689402626149], time: 112.233
mmmaddpg vs mmmaddpg steps: 874975, episodes: 35000, mean episode reward: 17.97997053601416, agent episode reward: [-23.10980337330351, 20.54488695465884, 20.54488695465884], time: 112.274
mmmaddpg vs mmmaddpg steps: 899975, episodes: 36000, mean episode reward: 16.243282983986386, agent episode reward: [-21.49180675626303, 18.86754487012471, 18.86754487012471], time: 113.17
mmmaddpg vs mmmaddpg steps: 924975, episodes: 37000, mean episode reward: 18.888855675579553, agent episode reward: [-25.61537011391163, 22.252112894745594, 22.252112894745594], time: 112.888
mmmaddpg vs mmmaddpg steps: 949975, episodes: 38000, mean episode reward: 14.697707806376156, agent episode reward: [-26.73833264451324, 20.7180202254447, 20.7180202254447], time: 113.921
mmmaddpg vs mmmaddpg steps: 974975, episodes: 39000, mean episode reward: 10.14668601286042, agent episode reward: [-27.979669654569022, 19.063177833714718, 19.063177833714718], time: 112.308
mmmaddpg vs mmmaddpg steps: 999975, episodes: 40000, mean episode reward: 20.237129355236004, agent episode reward: [-22.97485688337389, 21.605993119304948, 21.605993119304948], time: 111.36
mmmaddpg vs mmmaddpg steps: 1024975, episodes: 41000, mean episode reward: 15.805560271681514, agent episode reward: [-19.019892348023657, 17.412726309852584, 17.412726309852584], time: 112.307
mmmaddpg vs mmmaddpg steps: 1049975, episodes: 42000, mean episode reward: 18.08087288818411, agent episode reward: [-21.061310861165005, 19.571091874674558, 19.571091874674558], time: 113.094
mmmaddpg vs mmmaddpg steps: 1074975, episodes: 43000, mean episode reward: 9.765512170418631, agent episode reward: [-18.03363550715145, 13.89957383878504, 13.89957383878504], time: 113.494
mmmaddpg vs mmmaddpg steps: 1099975, episodes: 44000, mean episode reward: 14.24155338966064, agent episode reward: [-17.419963495563692, 15.830758442612169, 15.830758442612169], time: 114.327
mmmaddpg vs mmmaddpg steps: 1124975, episodes: 45000, mean episode reward: 11.940729294298766, agent episode reward: [-16.10539603747626, 14.023062665887515, 14.023062665887515], time: 113.84
mmmaddpg vs mmmaddpg steps: 1149975, episodes: 46000, mean episode reward: 10.358919681236012, agent episode reward: [-14.084620607218898, 12.221770144227454, 12.221770144227454], time: 113.743
mmmaddpg vs mmmaddpg steps: 1174975, episodes: 47000, mean episode reward: 9.383571595685595, agent episode reward: [-12.951537180699466, 11.167554388192531, 11.167554388192531], time: 112.565
mmmaddpg vs mmmaddpg steps: 1199975, episodes: 48000, mean episode reward: 7.776029636794608, agent episode reward: [-12.36800700871476, 10.072018322754683, 10.072018322754683], time: 114.053
mmmaddpg vs mmmaddpg steps: 1224975, episodes: 49000, mean episode reward: 6.954813999862186, agent episode reward: [-10.456891441877927, 8.705852720870055, 8.705852720870055], time: 112.204
mmmaddpg vs mmmaddpg steps: 1249975, episodes: 50000, mean episode reward: 6.554772669737875, agent episode reward: [-10.969506756030992, 8.762139712884434, 8.762139712884434], time: 112.748
mmmaddpg vs mmmaddpg steps: 1274975, episodes: 51000, mean episode reward: 6.490248387769621, agent episode reward: [-11.697576529552972, 9.093912458661299, 9.093912458661299], time: 112.929
mmmaddpg vs mmmaddpg steps: 1299975, episodes: 52000, mean episode reward: -1.716395413048017, agent episode reward: [-11.431811362232999, 4.857707974592492, 4.857707974592492], time: 114.13
mmmaddpg vs mmmaddpg steps: 1324975, episodes: 53000, mean episode reward: 1.7864473995534063, agent episode reward: [-12.12023199318157, 6.953339696367488, 6.953339696367488], time: 113.088
mmmaddpg vs mmmaddpg steps: 1349975, episodes: 54000, mean episode reward: 10.562319881565033, agent episode reward: [-14.573492881033426, 12.56790638129923, 12.56790638129923], time: 112.709
mmmaddpg vs mmmaddpg steps: 1374975, episodes: 55000, mean episode reward: 9.157783672689948, agent episode reward: [-14.666409097628494, 11.91209638515922, 11.91209638515922], time: 113.082
mmmaddpg vs mmmaddpg steps: 1399975, episodes: 56000, mean episode reward: 7.932143172252397, agent episode reward: [-13.87421423050709, 10.903178701379744, 10.903178701379744], time: 112.74
mmmaddpg vs mmmaddpg steps: 1424975, episodes: 57000, mean episode reward: 4.345737517182709, agent episode reward: [-13.299797161170114, 8.822767339176412, 8.822767339176412], time: 112.931
mmmaddpg vs mmmaddpg steps: 1449975, episodes: 58000, mean episode reward: 4.165251979371396, agent episode reward: [-17.419932781188972, 10.792592380280185, 10.792592380280185], time: 112.438
mmmaddpg vs mmmaddpg steps: 1474975, episodes: 59000, mean episode reward: 3.888750627575808, agent episode reward: [-20.117276921373364, 12.003013774474585, 12.003013774474585], time: 113.964
mmmaddpg vs mmmaddpg steps: 1499975, episodes: 60000, mean episode reward: 8.24582405174564, agent episode reward: [-12.812579618321815, 10.529201835033728, 10.529201835033728], time: 104.355
...Finished total of 60001 episodes.
